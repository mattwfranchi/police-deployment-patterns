{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96152a43",
   "metadata": {},
   "source": [
    "# Estimating Exposure to Police from Dashcam Data \n",
    "Matt Franchi, Jan 2023 \n",
    "\n",
    "This notebook contains all work needed to generate paper materials for the FAccT 2023 conference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d3da7",
   "metadata": {},
   "source": [
    "## 0. Module Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b4dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from zoneinfo import ZoneInfo\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib\n",
    "from scipy.stats import bootstrap\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aceaef1",
   "metadata": {},
   "source": [
    "## (Optional) Enable LaTeX font rendering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5a6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires local LaTeX installation \n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern Roman']})\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "rc('text.latex', preamble=r'\\usepackage{amsmath} \\usepackage[T1]{fontenc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8a196",
   "metadata": {},
   "source": [
    "## Global Constants "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399b925",
   "metadata": {},
   "source": [
    "### I/O Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884b9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANL_DATASET_PATH = \"/share/pierson/nexar_data/nexar_yolov7/intermediate_notebooks/analysis_dataset.csv\"\n",
    "FIRST_CHUNK_PATH = \"/share/pierson/nexar_data/nypd-deployment-patterns/output/1603771200000.csv\"\n",
    "VALSET_PATH = \"/share/pierson/nexar_data/dashcam-analysis/final_model_metrics/valset_2.csv\"\n",
    "TESTSET_PATH = \"/share/pierson/nexar_data/nexar_yolov7/test_set.csv\"\n",
    "PAPER_GIT_REPO_PATH = \"/share/pierson/nexar_data/nexar_yolov7/facct23-policing-disparities-paper/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1c45d5",
   "metadata": {},
   "source": [
    "### Geographic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b0f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "WGS = 'EPSG:4326'\n",
    "PROJ_CRS = 'EPSG:2263'\n",
    "NYC_COUNTY_CODES = ['005', '047', '061', '081', '085']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8692b2",
   "metadata": {},
   "source": [
    "### Analysis Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0884fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CHUNKS_PATH = '/share/pierson/nexar_data/FINAL_CHUNKS/%i.csv'\n",
    "COLS_TO_DEDUPLICATE_ON = ['lat', 'lng', 'timestamp'] # columns to use to check for duplicates\n",
    "MIN_DATE_FOR_DEMOGRAPHIC_ANALYSIS = datetime.datetime(2020, 10, 5, 0, 0, 0, tzinfo=ZoneInfo('US/Eastern')) # don't use data before this data to analyze disparities / demographics\n",
    "POSITIVE_CLASSIFICATION_THRESHOLD = 0.770508 # threshold to define a positive prediction\n",
    "LOCATION_COL_TO_GROUP_ON = 'GEOID20' # This should be the name of the column we're analyzing location grouping at - e.g., corresponding to Census Block Group or Census tract. CHECKED\n",
    "TOTAL_POPULATION_COL = 'Estimate_Total' # needs to match whether using Census tract or Block group. [Answer: CBG]\n",
    "WHITE_POPULATION_COL = 'Estimate_Total_Not_Hispanic_or_Latino_White_alone'\n",
    "BLACK_POPULATION_COL = 'Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone'\n",
    "ASIAN_POPULATION_COL = 'Estimate_Total_Not_Hispanic_or_Latino_Asian_alone'\n",
    "HISPANIC_POPULATION_COL = 'Estimate_Total_Hispanic_or_Latino'\n",
    "POPULATION_COUNT_COLS = [WHITE_POPULATION_COL, BLACK_POPULATION_COL, ASIAN_POPULATION_COL, HISPANIC_POPULATION_COL, TOTAL_POPULATION_COL]\n",
    "TIME_AND_DATE_COL = 'time_and_date_of_image'\n",
    "DEMOGRAPHIC_COLS = ['density_cbg', # things we want to look at correlations with. Demographic cols may not be best name. \n",
    "                    'black_frac',\n",
    "                    'white_frac', \n",
    "                    'distance_from_nearest_crime_6hr',\n",
    "                    'distance_from_nearest_police_station',\n",
    "                    'median_household_income']\n",
    "PREDICTION_COLS = ['above_threshold', 'calibrated_prediction', 'prediction_adjusted_for_police_station_distance'] # columns with police car predictions. We define these\n",
    "MIN_POPULATION_IN_AREA = 500\n",
    "BOROUGH_COL = 'boroname'\n",
    "NEIGHBORHOOD_COL = 'ntaname'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9d048",
   "metadata": {},
   "source": [
    "## 1. Dataset Verification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04e0365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in annotated dataset from disk \n",
    "#d = pd.concat([chunk for chunk in tqdm(pd.read_csv(ANL_DATASET_PATH, chunksize=100000), total=221, desc='Loading data')])\n",
    "# Variant: no progress bar, use faster pyarrow engine\n",
    "d = pd.read_csv(ANL_DATASET_PATH, engine='pyarrow')\n",
    "\n",
    "# ALT. Only load first chunk. \n",
    "#d = pd.concat([chunk for chunk in tqdm(pd.read_csv(FIRST_CHUNK_PATH, chunksize=100000), total=5, desc='Loading data')])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3aee7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994fdbcb",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43850d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dt column to EST timezone \n",
    "d.time_and_date_of_image = pd.to_datetime(d.time_and_date_of_image)\n",
    "d.time_and_date_of_image = d.time_and_date_of_image.dt.tz_convert('US/Eastern')\n",
    "print(\"Descriptive stats for datetimes in dataset.\")\n",
    "print(d.time_and_date_of_image.describe(datetime_is_numeric=True))\n",
    "print('â”€' * 50)\n",
    "\n",
    "\n",
    "# Inspect columns \n",
    "print(\"Columns in d: \")\n",
    "pprint(list(d.columns.values), width=120, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd1d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates. \n",
    "duplicate_idxs = d.duplicated(subset=COLS_TO_DEDUPLICATE_ON)\n",
    "print(\"warning: %i duplicates identified using %s, fraction %2.6f of rows; dropping rows\" % (duplicate_idxs.sum(), COLS_TO_DEDUPLICATE_ON, duplicate_idxs.mean()))\n",
    "d = d.loc[~duplicate_idxs].copy()\n",
    "\n",
    "def household_income_map(x):\n",
    "    if x == '-' or x == '':\n",
    "        return None\n",
    "    elif x == '250,000+':\n",
    "        return 250000\n",
    "    elif x == '2,500-':\n",
    "        return 2500\n",
    "    return float(x)\n",
    "\n",
    "d['GeoID'] = d['GeoID'].astype(str)\n",
    "\n",
    "# define Census variables\n",
    "d['median_household_income'] = d['median_household_income'].map(household_income_map)\n",
    "d['white_frac'] = d[WHITE_POPULATION_COL] / d[TOTAL_POPULATION_COL]\n",
    "d['black_frac'] = d[BLACK_POPULATION_COL] / d[TOTAL_POPULATION_COL]\n",
    "assert d['white_frac'].dropna().max() <= 1\n",
    "assert d['white_frac'].dropna().min() >= 0\n",
    "assert d['black_frac'].dropna().max() <= 1\n",
    "assert d['black_frac'].dropna().min() >= 0\n",
    "\n",
    "# define time variables\n",
    "#d['date'] = d[TIME_AND_DATE_COL].map(lambda x:datetime.datetime.strptime(x.split()[0], '%Y-%m-%d'))\n",
    "d['date'] = d[TIME_AND_DATE_COL].dt.date\n",
    "locations_by_date = d.groupby('date')[LOCATION_COL_TO_GROUP_ON].nunique()\n",
    "print('unique locations by', locations_by_date)\n",
    "\n",
    "# filter for dates with full coverage. \n",
    "print(\"In demographic analysis, filtering for locations after %s because more geographically representative\" % MIN_DATE_FOR_DEMOGRAPHIC_ANALYSIS)\n",
    "d_for_demo_analysis = d.loc[d['phase'] == 1].copy()\n",
    "print(\"%i/%i rows remaining\" % (len(d_for_demo_analysis), len(d)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d428bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILLING IN NA DATA \n",
    "d.conf.fillna(0, inplace=True)\n",
    "\n",
    "d.distance_from_nearest_police_station.fillna(0, inplace=True)\n",
    "d.distance_from_nearest_crime_1hr.fillna(0,inplace=True)\n",
    "d.distance_from_nearest_crime_3hr.fillna(0,inplace=True)\n",
    "d.distance_from_nearest_crime_6hr.fillna(0,inplace=True)\n",
    "\n",
    "d.density_cbg.fillna(0, inplace=True)\n",
    "d[\"Estimate_Total\"].fillna(0, inplace=True)\n",
    "d[\"Estimate_Total_Not_Hispanic_or_Latino_White_alone\"].fillna(0, inplace=True)\n",
    "d[\"Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone\"].fillna(0,inplace=True)\n",
    "d[\"Estimate_Total_Not_Hispanic_or_Latino_Asian_alone\"].fillna(0, inplace=True)\n",
    "d[\"Estimate_Total_Hispanic_or_Latino\"].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e71ad",
   "metadata": {},
   "source": [
    "### Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90821227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all lng/lat coordinates are in range \n",
    "LNG_BOUNDS = (-78,-73)\n",
    "LAT_BOUNDS = (40, 45)\n",
    "\n",
    "lng_in_range = ((d.lng > LNG_BOUNDS[0]) & (d.lng < LNG_BOUNDS[1]))\n",
    "print(f\"{sum(lng_in_range)} / {len(d.index)} have in range longitudes.\")\n",
    "lat_in_range = ((d.lat > LAT_BOUNDS[0]) & (d.lat < LAT_BOUNDS[1]))\n",
    "print(f\"{sum(lat_in_range)} / {len(d.index)} have in range latitudes.\")\n",
    "\n",
    "assert lng_in_range.all()\n",
    "assert lat_in_range.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97db9157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data -- set threshold, print out columns with more than this percent missing \n",
    "NA_THRESHOLD = 0.025\n",
    "print(f\"Dataset columns with > {NA_THRESHOLD} proportion of missing images.\")\n",
    "pprint(d.loc[:, d.isnull().mean() > NA_THRESHOLD].isnull().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cae81008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that all core analysis columns fall within sensible value ranges \n",
    "CONF_BOUNDS = (0,1)\n",
    "\n",
    "NTANAMES_LENGTH = 195 \n",
    "DISTANCE_FROM_NEAREST_POLICE_STATION_BOUNDS=(0,50000)\n",
    "DISTANCE_FROM_NEAREST_CRIME_BOUNDS=(0,500000)\n",
    "MEDIAN_HOUSEHOLD_INCOME_BOUNDS=(0,100000000)\n",
    "ESTIMATE_WHITE_BOUNDS = (0, 10000000)\n",
    "ESTIMATE_BLACK_BOUNDS = (0, 10000000)\n",
    "ESTIMATE_ASIAN_BOUNDS = (0, 10000000)\n",
    "ESTIMATE_HISPANIC_BOUNDS = (0, 10000000)\n",
    "DENSITY_BOUNDS = (0,10000000)\n",
    "TIME_AND_DATE_OF_IMAGE_BOUNDS = (datetime.datetime(2020,3,1,0,0,0,tzinfo=ZoneInfo('US/Eastern')), datetime.datetime(2020,11,17,0,0,0,tzinfo=ZoneInfo('US/Eastern')))\n",
    "HOUR_BOUNDS = (0,23)\n",
    "DAY_OF_WEEK_BOUNDS = (0,6)\n",
    "DAY_OF_MONTH_BOUNDS = (1,31)\n",
    "WEEKEND_BOUNDS = (0,1)\n",
    "NIGHTTIME_BOUNDS = (0,1)\n",
    "\n",
    "# Model features\n",
    "assert (d.conf >= CONF_BOUNDS[0]).all()\n",
    "assert (d.conf < CONF_BOUNDS[1]).all()\n",
    "\n",
    "# Demographic features \n",
    "#assert d.median_household_income > MEDIAN_HOUSEHOLD_INCOME_BOUNDS[0]\n",
    "#assert d.median_household_income < MEDIAN_HOUSEHOLD_INCOME_BOUNDS[1]\n",
    "\n",
    "assert (d.density_cbg >= DENSITY_BOUNDS[0]).all()\n",
    "assert (d.density_cbg <= DENSITY_BOUNDS[1]).all()\n",
    "\n",
    "assert (d[\"Estimate_Total_Not_Hispanic_or_Latino_White_alone\"] >= ESTIMATE_WHITE_BOUNDS[0]).all()\n",
    "assert (d[\"Estimate_Total_Not_Hispanic_or_Latino_White_alone\"] <= ESTIMATE_WHITE_BOUNDS[1]).all()\n",
    "\n",
    "assert (d[\"Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone\"] >= ESTIMATE_BLACK_BOUNDS[0]).all()\n",
    "assert (d[\"Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone\"] <= ESTIMATE_BLACK_BOUNDS[1]).all()\n",
    "\n",
    "#d[\"Estimate_Total_Not_Hispanic_or_Latino_Asian_alone\"]\n",
    "\n",
    "assert (d[\"Estimate_Total_Not_Hispanic_or_Latino_Asian_alone\"] >= ESTIMATE_ASIAN_BOUNDS[0]).all()\n",
    "assert (d[\"Estimate_Total_Not_Hispanic_or_Latino_Asian_alone\"] <= ESTIMATE_ASIAN_BOUNDS[1]).all()\n",
    "\n",
    "assert (d[\"Estimate_Total_Hispanic_or_Latino\"] >= ESTIMATE_HISPANIC_BOUNDS[0]).all()\n",
    "assert (d[\"Estimate_Total_Hispanic_or_Latino\"] <= ESTIMATE_HISPANIC_BOUNDS[1]).all()\n",
    "\n",
    "# Distance features \n",
    "assert (d.distance_from_nearest_police_station >= DISTANCE_FROM_NEAREST_POLICE_STATION_BOUNDS[0]).all()\n",
    "assert (d.distance_from_nearest_police_station <= DISTANCE_FROM_NEAREST_POLICE_STATION_BOUNDS[1]).all()\n",
    "\n",
    "assert (d.distance_from_nearest_crime_1hr >= DISTANCE_FROM_NEAREST_CRIME_BOUNDS[0]).all()\n",
    "assert (d.distance_from_nearest_crime_1hr <= DISTANCE_FROM_NEAREST_CRIME_BOUNDS[1]).all()\n",
    "\n",
    "assert (d.distance_from_nearest_crime_3hr >= DISTANCE_FROM_NEAREST_CRIME_BOUNDS[0]).all()\n",
    "assert (d.distance_from_nearest_crime_3hr <= DISTANCE_FROM_NEAREST_CRIME_BOUNDS[1]).all()\n",
    "\n",
    "assert (d.distance_from_nearest_crime_6hr >= DISTANCE_FROM_NEAREST_CRIME_BOUNDS[0]).all()\n",
    "assert (d.distance_from_nearest_crime_6hr <= DISTANCE_FROM_NEAREST_CRIME_BOUNDS[1]).all()\n",
    "\n",
    "# Temporal features \n",
    "assert (d.time_and_date_of_image >= TIME_AND_DATE_OF_IMAGE_BOUNDS[0]).all()\n",
    "assert (d.time_and_date_of_image <= TIME_AND_DATE_OF_IMAGE_BOUNDS[1]).all()\n",
    "\n",
    "assert (d.hour >= HOUR_BOUNDS[0]).all()\n",
    "assert (d.hour <= HOUR_BOUNDS[1]).all()\n",
    "\n",
    "assert (d.day_of_week >= DAY_OF_WEEK_BOUNDS[0]).all()\n",
    "assert (d.day_of_week <= DAY_OF_WEEK_BOUNDS[1]).all()\n",
    "\n",
    "assert (d.day_of_month >= DAY_OF_MONTH_BOUNDS[0]).all()\n",
    "assert (d.day_of_month <= DAY_OF_MONTH_BOUNDS[1]).all()\n",
    "\n",
    "assert (d.weekend >= WEEKEND_BOUNDS[0]).all()\n",
    "assert (d.weekend <= WEEKEND_BOUNDS[1]).all()\n",
    "\n",
    "assert (d.nighttime >= NIGHTTIME_BOUNDS[0]).all()\n",
    "assert (d.nighttime <= NIGHTTIME_BOUNDS[1]).all()\n",
    "\n",
    "print(\"We are sane! The dataset makes sense.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6a67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_anl_vars = ['distance_from_nearest_police_station','distance_from_nearest_crime_1hr','distance_from_nearest_crime_3hr','distance_from_nearest_crime_6hr',\n",
    "                 'Estimate_Total_Not_Hispanic_or_Latino_White_alone', 'Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone','ntaname','time_and_date_of_image','hour','month','nighttime','day_of_month','day_of_week',\n",
    "                'density_cbg','median_household_income','boroct2020']\n",
    "\n",
    "core_anl_check = d[core_anl_vars]\n",
    "core_anl_check.describe(datetime_is_numeric=True).apply(lambda s: s.apply('{0:.2f}'.format))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50e860",
   "metadata": {},
   "source": [
    "## 2. Loading in Validation & Test Sets, External Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6aeabe",
   "metadata": {},
   "source": [
    "### Validation, Test Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cf0f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pd.read_csv(VALSET_PATH)\n",
    "t = pd.read_csv(TESTSET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9410ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgdf = gpd.GeoDataFrame(v, geometry=gpd.points_from_xy(v.lng, v.lat), crs=WGS)\n",
    "vgdf = vgdf.to_crs(PROJ_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e640c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgdf = gpd.GeoDataFrame(t, geometry=gpd.points_from_xy(t.lng, t.lat), crs=WGS)\n",
    "tgdf = tgdf.to_crs(PROJ_CRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516d63f",
   "metadata": {},
   "source": [
    "### NYC Neighborhood Tabulation Areas (NTAs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c342de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_ntas = gpd.read_file(\"/share/pierson/nexar_data/5_other_datasets/nynta2020_22c\")\n",
    "nyc_ntas = nyc_ntas.to_crs(PROJ_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "914f423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_ntas.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428a8c5",
   "metadata": {},
   "source": [
    "### NYC Census Block Groups (CBGs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fb96626",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_cbgs = gpd.read_file('/share/pierson/nexar_data/5_other_datasets/tl_2020_36_all/tl_2020_36_bg20.shp')\n",
    "ny_cbgs = ny_cbgs.to_crs(WGS)\n",
    "\n",
    "nyc_cbgs = ny_cbgs[ny_cbgs.COUNTYFP20.isin(NYC_COUNTY_CODES)]\n",
    "nyc_cbgs.reset_index(inplace=True)\n",
    "nyc_cbgs = nyc_cbgs.to_crs(PROJ_CRS)\n",
    "nyc_cbgs.GEOID20 = pd.to_numeric(nyc_cbgs.GEOID20)\n",
    "nyc_cbgs.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4cd2a",
   "metadata": {},
   "source": [
    "### NYC Zoning Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e0d56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoning Tests \n",
    "nyc_zoning = gpd.read_file(\"/share/pierson/nexar_data/5_other_datasets/nycgiszoningfeatures_202212shp\")\n",
    "nyc_zoning = nyc_zoning.to_crs('EPSG:2263')\n",
    "def residential(z): \n",
    "    if 'R' in z:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def commercial(z): \n",
    "    if 'C' in z: \n",
    "        return True \n",
    "    else: \n",
    "        return False \n",
    "\n",
    "def manufacturing(z): \n",
    "    if 'M' in z:\n",
    "        return True\n",
    "    else: \n",
    "        return False \n",
    "\n",
    "def high_level_zoning(z): \n",
    "    if 'R' in z: \n",
    "        return 'R'\n",
    "    elif 'C' in z:\n",
    "        return 'C'\n",
    "    elif 'M' in z:\n",
    "        return 'M'\n",
    "    \n",
    "nyc_zoning['high_level_zone'] = nyc_zoning.ZONEDIST.map(lambda z: high_level_zoning(z))\n",
    "\n",
    "\n",
    "nyc_zoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a59a0",
   "metadata": {},
   "source": [
    "### NYPD Precinct Locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2079335",
   "metadata": {},
   "outputs": [],
   "source": [
    "precincts = pd.read_csv(\"/share/pierson/nexar_data/5_other_datasets/nypd_precinct_locs.csv\")\n",
    "precincts_gdf = gpd.GeoDataFrame(precincts, geometry=gpd.points_from_xy(precincts.lng, precincts.lat), crs=WGS)\n",
    "precincts_gdf = precincts_gdf.to_crs(PROJ_CRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d52dc",
   "metadata": {},
   "source": [
    "### NYC Borough Boundaries (NYBB) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ae31d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "nybb = gpd.read_file(gpd.datasets.get_path('nybb'))\n",
    "nybb = nybb.to_crs(PROJ_CRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886f291",
   "metadata": {},
   "source": [
    "### NYC Arrests Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a968b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_arrests = pd.read_csv(\"/share/pierson/nexar_data/5_other_datasets/NYPD_Arrests_Data__Historic_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ead0623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_arrests = gpd.GeoDataFrame(nyc_arrests, geometry=gpd.points_from_xy(nyc_arrests.Longitude, nyc_arrests.Latitude), crs=WGS)\n",
    "nyc_arrests = nyc_arrests.to_crs(PROJ_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63ade84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests_by_nta = gpd.sjoin(nyc_arrests,nyc_ntas).groupby('NTAName').agg('size').to_frame('num_arrests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0028f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_ntas = nyc_ntas.merge(arrests_by_nta, left_on='NTAName', right_on='NTAName')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d928875",
   "metadata": {},
   "source": [
    "## 3. NYPD Deployment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcdc6b0",
   "metadata": {},
   "source": [
    "### Computing Probability Measures with Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10e39fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_probabilities_using_valset(v, d_to_add_prediction_columns_to):\n",
    "    \"\"\"\n",
    "    Annotate a dataframe, d_to_add_prediction_columns_to, with three prediction columns\n",
    "    derived from the val set v.\n",
    "    \n",
    "    1. A simple binary variable with whether conf > POSITIVE_CLASSIFICATION_THRESHOLD\n",
    "    2. A probabilistic prediction from val set: if above threshold, Pr(ground truth positive | above threshold in val set)\n",
    "    and if below threshold, Pr(ground truth negative | below threshold in val set)\n",
    "    3. A probability adjusted for police station distance. Not sure if this is a good thing to use, and should definitely check it is calibrated on test set if we do.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. annotate with simple binary score\n",
    "    assert v['Model_predicted_score'].isnull().sum() == 0\n",
    "    v['classified_positive'] = v['Model_predicted_score'] > POSITIVE_CLASSIFICATION_THRESHOLD\n",
    "    d_to_add_prediction_columns_to['above_threshold'] = (d_to_add_prediction_columns_to['conf'] > POSITIVE_CLASSIFICATION_THRESHOLD) * 1.\n",
    "    \n",
    "    # 2. compute probabilities given above/below threshold from val set\n",
    "    p_positive_given_classified_positive = v.loc[v['classified_positive'] == True, 'ground_truth'].mean()\n",
    "    p_positive_given_classified_negative = v.loc[v['classified_positive'] == False, 'ground_truth'].mean()\n",
    "    print(\"Fraction of val set classified positive: %2.3f (%i rows)\" % \n",
    "          (v['classified_positive'].mean(), v['classified_positive'].sum()))\n",
    "    print(\"Pr(true positive | classified positive): %2.3f\" % p_positive_given_classified_positive)\n",
    "    print(\"Pr(true positive | classified negative): %2.3f\" % p_positive_given_classified_negative)\n",
    "    d_to_add_prediction_columns_to['calibrated_prediction'] = d_to_add_prediction_columns_to['above_threshold'].map(lambda x:p_positive_given_classified_positive if x == 1 else p_positive_given_classified_negative) \n",
    "    \n",
    "    # 3. compute adjusted probability given police station distance. Not sure if this is necessary or wise, but adding just in case. \n",
    "    police_station_distance_model = sm.Logit.from_formula('ground_truth ~ Model_predicted_score + distance_from_nearest_police_station', data=v).fit()\n",
    "    print(police_station_distance_model.summary())\n",
    "    d_to_add_prediction_columns_to['Model_predicted_score'] = 0 # compute police-distance adjusted probability on d_to_add_prediction_columns_to. \n",
    "    d_to_add_prediction_columns_to.loc[~pd.isnull(d_to_add_prediction_columns_to['conf']), 'Model_predicted_score'] = d_to_add_prediction_columns_to['conf'].loc[~pd.isnull(d_to_add_prediction_columns_to['conf'])]\n",
    "    assert d_to_add_prediction_columns_to['Model_predicted_score'].isnull().sum() == 0\n",
    "    d_to_add_prediction_columns_to['prediction_adjusted_for_police_station_distance'] = police_station_distance_model.predict(d_to_add_prediction_columns_to).values\n",
    "    del d_to_add_prediction_columns_to['Model_predicted_score']\n",
    "    \n",
    "    added_cols = ['above_threshold', 'calibrated_prediction', 'prediction_adjusted_for_police_station_distance']\n",
    "    assert pd.isnull(d_to_add_prediction_columns_to[added_cols]).values.sum() == 0\n",
    "    assert (d_to_add_prediction_columns_to[added_cols].values < 0).sum() == 0\n",
    "    assert (d_to_add_prediction_columns_to[added_cols].values > 1).sum() == 0\n",
    "    for col in added_cols:\n",
    "        print(\"Mean value of prediction column %s: %2.3f; std %2.3f; > 0 %2.3f\" % (\n",
    "            col,\n",
    "            d_to_add_prediction_columns_to[col].mean(), \n",
    "            d_to_add_prediction_columns_to[col].std(), \n",
    "            (d_to_add_prediction_columns_to[col] > 0).mean()))\n",
    "    \n",
    "    return d_to_add_prediction_columns_to\n",
    "    \n",
    "d_for_demo_analysis = calibrate_probabilities_using_valset(v=v, d_to_add_prediction_columns_to=d)\n",
    "\n",
    "\n",
    "# PUT DGDF HERE \n",
    "# GeoDataFrame for d \n",
    "d_for_demo_analysis = gpd.GeoDataFrame(d_for_demo_analysis, geometry=gpd.points_from_xy(d_for_demo_analysis.lng, d_for_demo_analysis.lat), crs=WGS)\n",
    "d_for_demo_analysis = d_for_demo_analysis.to_crs(PROJ_CRS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ad57853",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d_for_demo_analysis.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e172978",
   "metadata": {},
   "source": [
    "### Geographic Aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52892159",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_nta = d_for_demo_analysis.groupby(['ntaname','NAME'])[POPULATION_COUNT_COLS + \n",
    "                                                              DEMOGRAPHIC_COLS + \n",
    "                                                             PREDICTION_COLS].agg('first')[[\"Estimate_Total_Not_Hispanic_or_Latino_White_alone\",\"Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone\"]].groupby(level='ntaname').agg('sum')\n",
    "pop_by_nta\n",
    "\n",
    "est_by_nta = d_for_demo_analysis.groupby(['ntaname'])[PREDICTION_COLS].agg('mean')\n",
    "est_by_nta\n",
    "\n",
    "nta_grouped_d = pop_by_nta.join(est_by_nta)\n",
    "nta_grouped_d = (nta_grouped_d, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b027f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by Census area. \n",
    "grouped_d = d_for_demo_analysis.groupby(LOCATION_COL_TO_GROUP_ON)[POPULATION_COUNT_COLS + \n",
    "                                                              DEMOGRAPHIC_COLS + \n",
    "                                                              PREDICTION_COLS].agg('mean')\n",
    "for col in POPULATION_COUNT_COLS + DEMOGRAPHIC_COLS: # check consistent values by location for demographics. Should only be one value of population count per Census area, for example. \n",
    "    if col in ['distance_from_nearest_crime_6hr', 'distance_from_nearest_police_station']:\n",
    "        continue\n",
    "    #assert d_for_demo_analysis.groupby(LOCATION_COL_TO_GROUP_ON)[col].nunique().map(lambda x:x in [0, 1]).all()\n",
    "\n",
    "print(\"%i unique Census areas using column %s\" % (len(grouped_d), LOCATION_COL_TO_GROUP_ON))\n",
    "print(\"Population statistics by area\")\n",
    "print(grouped_d[TOTAL_POPULATION_COL].describe([0.01, 0.05, 0.1, 0.5, 0.9, 0.99]))\n",
    "print(\"excluding census areas with population < %i keeps fraction %2.3f of population\" % \n",
    "      (MIN_POPULATION_IN_AREA, \n",
    "       grouped_d.loc[grouped_d[TOTAL_POPULATION_COL] >= MIN_POPULATION_IN_AREA, TOTAL_POPULATION_COL].sum()/grouped_d[TOTAL_POPULATION_COL].sum()))\n",
    "for col in POPULATION_COUNT_COLS: # sanity check that total counts look right. \n",
    "    print(\"summed values of %s: %i\" % (col, grouped_d[col].sum()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4387b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_d.calibrated_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd796b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nyc_cbgs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467bc36f",
   "metadata": {},
   "source": [
    "### Disparities Estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dc80193",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction_col in PREDICTION_COLS:\n",
    "    print(\"Using prediction col\", prediction_col)\n",
    "    estimates = {}\n",
    "    for demo_col in POPULATION_COUNT_COLS:\n",
    "        if demo_col == TOTAL_POPULATION_COL:\n",
    "            continue\n",
    "        # compute weighted mean as described in Census tract. \n",
    "        grouped_mean = (grouped_d[prediction_col] * grouped_d[demo_col]).sum()/grouped_d[demo_col].sum()\n",
    "        print(demo_col, grouped_mean)\n",
    "        estimates[demo_col] = grouped_mean\n",
    "    print(\"Ratio of Black estimate to white estimate: %2.3f\" % (estimates[BLACK_POPULATION_COL]/estimates[WHITE_POPULATION_COL]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d33897",
   "metadata": {},
   "source": [
    "## 4. Analysis Plots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8930fd",
   "metadata": {},
   "source": [
    "### Correlations Between All Measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aee0ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Coefficient\n",
    "pearson_corr = grouped_d.loc[grouped_d[TOTAL_POPULATION_COL] > MIN_POPULATION_IN_AREA, DEMOGRAPHIC_COLS].corr(method='pearson')\n",
    "mask = np.triu(np.ones_like(pearson_corr, dtype=bool))\n",
    "heatmap = sns.heatmap(pearson_corr, mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Pearson Correlations', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b043ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman Correlation Coefficient\n",
    "spearman_corr = grouped_d.loc[grouped_d[TOTAL_POPULATION_COL] > MIN_POPULATION_IN_AREA, DEMOGRAPHIC_COLS].corr(method='spearman')\n",
    "mask = np.triu(np.ones_like(spearman_corr, dtype=bool))\n",
    "heatmap = sns.heatmap(spearman_corr, mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Spearman Correlations', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a6691",
   "metadata": {},
   "source": [
    "### Breakdown by Neighborhood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a63c7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in PREDICTION_COLS:\n",
    "    print(\"\\n\\nneighborhoods with highest mean values of %s\" % col)\n",
    "    print(d_for_demo_analysis\n",
    "          .groupby(NEIGHBORHOOD_COL)[col]\n",
    "          .agg(['mean', 'size'])\n",
    "          .reset_index()\n",
    "          .sort_values(by='mean')[::-1])\n",
    "nta_breakdown = d_for_demo_analysis.groupby(NEIGHBORHOOD_COL)['calibrated_prediction'].agg(['mean', 'size']).reset_index().sort_values(by='mean')[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd6b6a2",
   "metadata": {},
   "source": [
    "## Map of Pr(police) by Census area (either neighborhood or Census tract). Can show this next to maps of density and potentially other variables - we decided to make this by neighborhood for a number of reasons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3112d",
   "metadata": {},
   "source": [
    "### Plot Data Loaded from Bootstraps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba28d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "nta_data = json.load(open(\"/share/pierson/nexar_data/bootstraps_for_matt/neighborhood_bootstraps.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "723ae3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_rta_means = pd.DataFrame.from_dict(nta_data['point_estimate'], orient='index')\n",
    "nta_rta_means = nta_rta_means[nta_rta_means.index.map(lambda x: 'relative_to_average' in x)]\n",
    "nta_rta_means.index = nta_rta_means.index.str.replace('_relative_to_average','')\n",
    "nta_rta_means.columns = ['Pr_police_rta']\n",
    "nta_rta_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba7fd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_ntas = nyc_ntas.merge(nta_rta_means, left_on='NTAName', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7656a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 0.25, 0.5, 0.75, 1, 2, 3,  4, 5]\n",
    "#labels = ['( 0, 0.5 ]', '( 0.5, 1 ]', '( 1, 2 ]', '> 2']\n",
    "nyc_ntas['Pr_police_rta'].fillna(0, inplace=True)\n",
    "nyc_ntas['pr_quantile'] = pd.cut(nyc_ntas['Pr_police_rta'], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3162a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24,24))\n",
    "colormap = 'bwr'\n",
    "nybb.plot(ax=ax, color='gainsboro', edgecolor='grey')\n",
    "nyc_ntas.plot(column='pr_quantile', ax=ax, cmap=colormap, legend=False)#, legend_kwds={'loc': 'upper left', 'title': 'Police deployment\\n(relative to city average)', 'ncols':2, 'fontsize':50, 'markerscale':3, 'title_fontsize':60, 'alignment':'center'})\n",
    "\n",
    "n = 16 # how many lines to draw or number of discrete color levels\n",
    "\n",
    "cmap = plt.get_cmap(colormap)\n",
    "\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=5)\n",
    "stretched_bounds = np.interp(np.linspace(0, 1, 257), np.linspace(0, 1, 16), [0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5])\n",
    "# normalize stretched bound values\n",
    "norm = matplotlib.colors.BoundaryNorm(stretched_bounds, ncolors=256)\n",
    "scalarmap = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "#sm.set_array([])\n",
    "cb = plt.colorbar(scalarmap, ticks=np.arange(0,n), ax=ax, orientation='horizontal', ticklocation='bottom', pad=0, anchor=(0.2,6), shrink=0.35)\n",
    "cb.ax.xaxis.set_label_position('top')\n",
    "cb.set_label(label='Police deployment\\n(relative to city average)',size=40,weight='bold', labelpad=20)\n",
    "cb.set_ticks([0, 0.25, 0.5, 0.75, 1, 2, 3,  4, 5], labels=[r'${0\\scriptstyle\\times}$', r'${0.25\\scriptstyle\\times}$', r'${0.5\\scriptstyle\\times}$', r'${0.75\\scriptstyle\\times}$', r'${1\\scriptstyle\\times}$', r'${2\\scriptstyle\\times}$', r'${3\\scriptstyle\\times}$', r'${4\\scriptstyle\\times}$', r'${5\\scriptstyle\\times}$'],size=25)\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PAPER_GIT_REPO_PATH}/figures/Pr_police_rta_ntas.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26637253",
   "metadata": {},
   "source": [
    "### VARIANT: Un-bootstrapped Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d73fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "try: \n",
    "    d_for_demo_analysis.drop(['index_right'],axis=1,inplace=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass \n",
    "\n",
    "\n",
    "try: \n",
    "    d_for_demo_analysis.drop(['index'],axis=1,inplace=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "     \n",
    "\n",
    "try: \n",
    "    d_for_demo_analysis.drop(['index_left'],axis=1,inplace=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "dgdf_for_demo_by_ntas = gpd.sjoin(nyc_ntas, d_for_demo_analysis, how='left', predicate='contains').groupby('NTAName').agg('mean','size')\n",
    "\n",
    "nyc_ntas_proj_demo = nyc_ntas.merge(dgdf_for_demo_by_ntas,left_on='NTAName',right_on='NTAName')\n",
    "nyc_ntas_proj_demo.calibrated_prediction.fillna(0, inplace=True)\n",
    "nyc_ntas_proj_demo.calibrated_prediction_decile = pd.qcut(nyc_ntas_proj_demo.calibrated_prediction, 5, duplicates='drop')\n",
    "print(nyc_ntas_proj_demo.calibrated_prediction.describe())\n",
    "\n",
    "\n",
    "\n",
    "nybb.plot(ax=ax, edgecolor='grey', color='w')\n",
    "nyc_ntas_proj_demo.plot(column=nyc_ntas_proj_demo.calibrated_prediction_decile, ax=ax, cmap='cividis', legend=True, legend_kwds={'title':'Calibrated Probability of Police Exposure'})\n",
    "\n",
    "\n",
    "#nyc_ntas.plot(color='blue', ax=ax)\n",
    "\n",
    "#nyc_ntas.merge(nta_grouped_d, left_on='NTAName', right_on='ntaname').fillna(0).plot(column='calibrated_prediction', ax=ax, cmap='plasma', legend=True, legend_kwds={\"location\": \"bottom\", \"shrink\": 0.5, \"pad\": 0, 'label': 'Calibrated Probability of Police Exposure'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f\"{PAPER_GIT_REPO_PATH}/figures/Pr_police_by_nta.jpg\", dpi=450)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e136f",
   "metadata": {},
   "source": [
    "### VARIANT2: More granular, exposure by CBG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2d05cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgdf_for_demo_by_cbgs = gpd.sjoin(nyc_cbgs, d_for_demo_analysis, how='left', predicate='contains').groupby('index').agg('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d47f54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "nyc_cbgs_proj_demo = nyc_cbgs.merge(dgdf_for_demo_by_cbgs,left_on='GEOID20',right_on='GEOID20_left')\n",
    "nyc_cbgs_proj_demo.calibrated_prediction.fillna(0, inplace=True)\n",
    "nyc_cbgs_proj_demo.calibrated_prediction_decile = pd.qcut(nyc_cbgs_proj_demo.calibrated_prediction, 18, duplicates='drop')\n",
    "print(nyc_cbgs_proj_demo.calibrated_prediction_decile)\n",
    "nyc_cbgs_proj_demo.plot(column=nyc_cbgs_proj_demo.calibrated_prediction_decile, ax=ax, cmap='cividis', legend=True)#,legend_kwds={\"location\": \"bottom\", \"shrink\": 0.5, \"pad\": 0, 'label': 'Calibrated Probability of Police Exposure'})\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('PR_police_by_cbg.jpg', dpi=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbdbaec",
   "metadata": {},
   "source": [
    "### Table of neighborhoods with highest police levels  (include borough as a column as well assuming that each neighborhood is only in one borough). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10cbf3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nyc_cbgs_proj_demo['Estimate_Total'].sum())\n",
    "\n",
    "nta_breakdown_top10 = nyc_ntas_proj_demo.sort_values(by='calibrated_prediction')[::-1][:10][[\"NTAName\", \"BoroName\", \"calibrated_prediction\"]]\n",
    "\n",
    "rename = {\"NTAName\": \"Neighborhood\", \"BoroName\": \"Borough\", \"calibrated_prediction\": \"Calibrated Probability of Police Exposure\"}\n",
    "\n",
    "nta_breakdown_top10.rename(columns=rename, inplace=True)\n",
    "\n",
    "nta_breakdown_top10.to_latex(f'{PAPER_GIT_REPO_PATH}/tables/nta_breakdown_top10.tex', index=False, float_format=\"%.2f\")\n",
    "\n",
    "nta_breakdown_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590925c",
   "metadata": {},
   "source": [
    "### Table of police levels by borough (currently this is showing big disparities for Manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e2e64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "boro_breakdown = d_for_demo_analysis.groupby('boroname')['calibrated_prediction'].agg(['mean']).sort_values(by='mean')[::-1]\n",
    "\n",
    "boro_populations = nyc_cbgs_proj_demo.groupby('borocode')['Estimate_Total'].agg('sum')\n",
    "\n",
    "boro_populations.index = boro_breakdown.index\n",
    "\n",
    "\n",
    "cols=['calibrated_prediction_mean', 'calibrated_prediction_size', 'calibrated_prediction_sum', 'total_population_mean', 'total_population_size', 'total_population_sum']\n",
    "\n",
    "rename = {\"mean\": \"Calibrated Probability of Police Exposure\", \"Index\": \"Borough\"}\n",
    "boro_breakdown = boro_breakdown.rename(columns=rename)\n",
    "#boro_breakdown.columns=boro_breakdown.columns.droplevel(0) \n",
    "#boro_breakdown.columns = cols\n",
    "\n",
    "boro_breakdown['Total Population'] = boro_populations\n",
    "boro_breakdown = boro_breakdown.rename_axis('Borough')\n",
    "\n",
    "\n",
    "boro_breakdown['Population Weighted Probability of Police'] = (boro_breakdown['Calibrated Probability of Police Exposure'] * boro_breakdown['Total Population']) / boro_breakdown['Total Population'].sum() \n",
    "boro_breakdown['Population Weighted Probability of Police, Relative to Mean'] = boro_breakdown['Population Weighted Probability of Police'] / boro_breakdown['Population Weighted Probability of Police'].mean()\n",
    "\n",
    "boro_breakdown = boro_breakdown['Population Weighted Probability of Police, Relative to Mean']\n",
    "boro_breakdown.to_latex(f'{PAPER_GIT_REPO_PATH}/tables/borough_breakdown.tex', index=True, float_format=\"%.2f\")    \n",
    "\n",
    "boro_breakdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14194ad7",
   "metadata": {},
   "source": [
    "### Police levels by zone (residential vs commercial etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58684df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zoning_demo = gpd.overlay(nyc_cbgs_proj_demo, nyc_zoning, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e1d0d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zoning_demo.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e0ac320",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_cbgs_proj_demo.geometry.area.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12328e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_cbgs_proj_demo.groupby('GEOID20').agg('first')[\"shape_area\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "931434bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zoning_demo['subarea'] = nyc_zoning_demo.geometry.area\n",
    "nyc_zoning_demo.subarea.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "021337a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "nyc_cbgs_proj_demo.plot(ax=ax, color='blue', alpha=0.5)\n",
    "nyc_zoning_demo.plot(ax=ax, color='red', alpha=0.5)\n",
    "nyc_zoning_demo.groupby('GEOID20').agg('first').plot(ax=ax, color='green', alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bc3b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zoning_demo.groupby('GEOID20').agg('first').geometry.area.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f385c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbgs_by_zone_prop = nyc_zoning_demo.groupby(['GEOID20','high_level_zone'])[\"subarea\"].agg('sum').unstack(level=1).fillna(0).div(nyc_cbgs_proj_demo.set_index('GEOID20').geometry.area, axis='rows') \n",
    "cbgs_by_zone_prop\n",
    "residential_cbgs = cbgs_by_zone_prop[cbgs_by_zone_prop.R > 0.9]\n",
    "commercial_cbgs = cbgs_by_zone_prop[cbgs_by_zone_prop.C > 0.9]\n",
    "manufacturing_cbgs = cbgs_by_zone_prop[cbgs_by_zone_prop.M > 0.9]\n",
    "\n",
    "\n",
    "print(commercial_cbgs)\n",
    "print(manufacturing_cbgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55bea53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "residential_grouped_d = grouped_d[grouped_d.index.isin(residential_cbgs.index)]\n",
    "commercial_grouped_d = grouped_d[grouped_d.index.isin(commercial_cbgs.index)]\n",
    "manufacturing_grouped_d = grouped_d[grouped_d.index.isin(manufacturing_cbgs.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e7b9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_for_demo_analysis.columns)\n",
    "#dgdf_for_demo_analysis.drop('index_right', axis=1, inplace=True)\n",
    "d_mapped_to_zones = gpd.sjoin(nyc_zoning, d_for_demo_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b0c0b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_by_zone = d_mapped_to_zones.groupby('ZONEDIST').agg('mean','size')[['calibrated_prediction','density_cbg']].sort_values(by='calibrated_prediction')[::-1]\n",
    "pr_by_zone.index.values\n",
    "m = [x for x in pr_by_zone.index.values if 'M' in x]\n",
    "c = [x for x in pr_by_zone.index.values if 'C' in x]\n",
    "r = [x for x in pr_by_zone.index.values if 'R' in x]\n",
    "# r also picks up 'PARK' and 'PLAYGROUND' which is a convenient catch in my mind \n",
    "print(m)\n",
    "print(c)\n",
    "print(r)\n",
    "\n",
    "m_pr = pr_by_zone[pr_by_zone.index.isin(m)]\n",
    "c_pr = pr_by_zone[pr_by_zone.index.isin(c)]\n",
    "r_pr = pr_by_zone[pr_by_zone.index.isin(r)]\n",
    "\n",
    "\n",
    "\n",
    "def zone_classifier(z): \n",
    "    if 'R' in z: \n",
    "        return 'R'\n",
    "    elif 'C' in z:\n",
    "        return 'C'\n",
    "    elif 'M' in z:\n",
    "        return 'M'\n",
    "\n",
    "print(m_pr.agg('mean'), c_pr.agg('mean'), r_pr.agg('mean'))\n",
    "print(c_pr.agg('mean') / r_pr.agg('mean'))\n",
    "\n",
    "pr_by_zone['type'] = pr_by_zone.index.map(lambda x: zone_classifier(x))\n",
    "pr_by_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "640bb271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def population_weighting(metric_to_weight, weights):\n",
    "    return (metric_to_weight * weights).sum() / weights.sum()\n",
    "\n",
    "#r_pr = ((residential_grouped_d.calibrated_prediction * residential_grouped_d['Estimate_Total:'])).sum() / r_pop\n",
    "#c_pr = (commercial_grouped_d.calibrated_prediction * (commercial_grouped_d['Estimate_Total:'] / c_pop)).mean()\n",
    "#m_pr = (manufacturing_grouped_d.calibrated_prediction * (manufacturing_grouped_d['Estimate_Total:'] / m_pop)).mean()\n",
    "\n",
    "r_pr = population_weighting(residential_grouped_d.calibrated_prediction, residential_grouped_d['Estimate_Total'])\n",
    "c_pr = population_weighting(commercial_grouped_d.calibrated_prediction, commercial_grouped_d['Estimate_Total'])\n",
    "m_pr = population_weighting(manufacturing_grouped_d.calibrated_prediction, manufacturing_grouped_d['Estimate_Total'])\n",
    "\n",
    "#r_pr = residential_grouped_d.calibrated_prediction.mean()\n",
    "#c_pr = commercial_grouped_d.calibrated_prediction.mean()\n",
    "#m_pr = manufacturing_grouped_d.calibrated_prediction.mean()\n",
    "\n",
    "data = {'Residential': r_pr, 'Commercial': c_pr, 'Manufacturing': m_pr}\n",
    "print(data)\n",
    "pr_by_zone_table = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "pr_by_zone_table = pr_by_zone_table.rename_axis('Zoning Type')\n",
    "pr_by_zone_table.columns = ['Population-Weighted Probability of Police Exposure']\n",
    "\n",
    "\n",
    "#pr_by_zone_table.to_latex(f'{PAPER_GIT_REPO_PATH}/tables/pr_by_zone_type.tex', float_format=\"%.2f\")\n",
    "\n",
    "pr_by_zone_table\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f34c14",
   "metadata": {},
   "source": [
    "### Police levels by race "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a7eaf2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction_col in PREDICTION_COLS:\n",
    "    print(\"Using prediction col\", prediction_col)\n",
    "    estimates = {}\n",
    "    for demo_col in POPULATION_COUNT_COLS:\n",
    "        if demo_col == TOTAL_POPULATION_COL:\n",
    "            continue\n",
    "        # compute weighted mean as described in Census tract. \n",
    "        grouped_mean = (grouped_d[prediction_col] * grouped_d[demo_col]).sum()/grouped_d[demo_col].sum()\n",
    "        print(demo_col, grouped_mean)\n",
    "        estimates[demo_col] = grouped_mean\n",
    "    print(\"Ratio of Black estimate to white estimate: %2.3f\" % (estimates[BLACK_POPULATION_COL]/estimates[WHITE_POPULATION_COL]))\n",
    "\n",
    "\n",
    "pr_by_race_table = pd.DataFrame.from_dict(estimates, orient='index')\n",
    "pr_by_race_table['Weighted Probability of Police, Relative to Mean'] = pr_by_race_table.iloc[:,0] / pr_by_race_table.iloc[:,0].mean()\n",
    "\n",
    "nice_names = {'Estimate_Total_Not_Hispanic_or_Latino_White_alone': 'White', 'Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone': 'Black/African American', 'Estimate_Total_Not_Hispanic_or_Latino_Asian_alone': 'Asian', 'Estimate_Total_Hispanic_or_Latino': 'Hispanic / Some other race'}\n",
    "pr_by_race_table.index = pr_by_race_table.index.map(lambda x: nice_names[x])\n",
    "pr_by_race_table = pr_by_race_table['Weighted Probability of Police, Relative to Mean']\n",
    "\n",
    "\n",
    "\n",
    "pr_by_race_table.to_latex(f'{PAPER_GIT_REPO_PATH}/tables/pr_by_race.tex', index=True, float_format=\"%.2f\")    \n",
    "pr_by_race_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a7e52",
   "metadata": {},
   "source": [
    "### Police levels by race (only residential zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c973cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction_col in PREDICTION_COLS:\n",
    "    print(\"Using prediction col\", prediction_col)\n",
    "    estimates = {}\n",
    "    for demo_col in POPULATION_COUNT_COLS:\n",
    "        if demo_col == TOTAL_POPULATION_COL:\n",
    "            continue\n",
    "        # compute weighted mean as described in Census tract. \n",
    "        grouped_mean = (residential_grouped_d[prediction_col] * residential_grouped_d[demo_col]).sum()/residential_grouped_d[demo_col].sum()\n",
    "        print(demo_col, grouped_mean)\n",
    "        estimates[demo_col] = grouped_mean\n",
    "    #print(\"Ratio of Black estimate to white estimate: %2.3f\" % (estimates[demo_col]/estimates[WHITE_POPULATION_COL]))\n",
    "\n",
    "\n",
    "pr_by_race_rzones_table = pd.DataFrame.from_dict(estimates, orient='index')\n",
    "pr_by_race_rzones_table['Weighted Probability of Police, Relative to Mean'] = pr_by_race_table.values\n",
    "pr_by_race_rzones_table['Weighted Probability of Police, Relative to Mean [R Zoning Only]'] = pr_by_race_rzones_table.iloc[:,0] / pr_by_race_rzones_table.iloc[:,0].mean()\n",
    "\n",
    "nice_names = {'Estimate_Total_Not_Hispanic_or_Latino_White_alone': 'White', 'Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone': 'Black/African American', 'Estimate_Total_Not_Hispanic_or_Latino_Asian_alone': 'Asian', 'Estimate_Total_Hispanic_or_Latino': 'Hispanic / Some other race'}\n",
    "pr_by_race_rzones_table.index = pr_by_race_rzones_table.index.map(lambda x: nice_names[x])\n",
    "pr_by_race_rzones_table = pr_by_race_rzones_table.iloc[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "pr_by_race_rzones_table.to_latex(f'{PAPER_GIT_REPO_PATH}/tables/pr_by_race_residential.tex', index=True, float_format=\"%.2f\")    \n",
    "\n",
    "\n",
    "pr_by_race_rzones_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd8666",
   "metadata": {},
   "source": [
    "### Arrest Rates Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d931f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_arrests = pd.read_csv(\"/share/pierson/nexar_data/5_other_datasets/NYPD_Arrests_Data__Historic_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da32c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_arrests = gpd.GeoDataFrame(nyc_arrests, geometry=gpd.points_from_xy(nyc_arrests.Longitude, nyc_arrests.Latitude), crs=WGS)\n",
    "nyc_arrests = nyc_arrests.to_crs(PROJ_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2eb057dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests_by_nta = gpd.sjoin(nyc_arrests,nyc_ntas).groupby('NTAName').agg('size').to_frame('num_arrests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c41c20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_ntas = nyc_ntas.merge(arrests_by_nta, left_on='NTAName', right_on='NTAName')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb67c6",
   "metadata": {},
   "source": [
    "## 5. Model Development & Evaluation Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05212e89",
   "metadata": {},
   "source": [
    "### P/R on V/T Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26a4e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics \n",
    "pandr_vt = pd.DataFrame()\n",
    "p = []\n",
    "r = []\n",
    "rows = ['Validation Set', 'Test Set']\n",
    "for s in [v, t]: \n",
    "\n",
    "\n",
    "    tp = (s['Model_predicted_score'] >= POSITIVE_CLASSIFICATION_THRESHOLD) & (s['ground_truth'] == 1)\n",
    "    fp = (s['Model_predicted_score'] >= POSITIVE_CLASSIFICATION_THRESHOLD) & (s['ground_truth'] == 0)\n",
    "\n",
    "    fn = (s['Model_predicted_score'] < POSITIVE_CLASSIFICATION_THRESHOLD) & (s['ground_truth'] == 1)\n",
    "    tn = (s['Model_predicted_score'] < POSITIVE_CLASSIFICATION_THRESHOLD) & (s['ground_truth'] == 0)\n",
    "    \n",
    "    p.append( tp.sum() / (tp.sum() + fp.sum()))\n",
    "    r.append( tp.sum() / (tp.sum() + fn.sum()))\n",
    "    \n",
    "    \n",
    "\n",
    "pandr_vt['Precision'] = p\n",
    "pandr_vt['Recall'] = r\n",
    "pandr_vt.index = rows \n",
    "\n",
    "pandr_vt.to_latex(f\"{PAPER_GIT_REPO_PATH}/tables/pandr_vt.tex\", float_format=\"%.2f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5945ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "perf_stats = pd.DataFrame() \n",
    "p = []\n",
    "r = []\n",
    "auc = []\n",
    "ap = []\n",
    "\n",
    "rows = ['Validation Set', 'Test Set']\n",
    "\n",
    "for s in [v, t]: \n",
    "    \n",
    "    tp = (s['Model_predicted_score'] >= POSITIVE_CLASSIFICATION_THRESHOLD) & (s['ground_truth'] == 1)\n",
    "    fp = (s['Model_predicted_score'] >= POSITIVE_CLASSIFICATION_THRESHOLD) & (s['ground_truth'] == 0)\n",
    "\n",
    "    fn = (s['Model_predicted_score'] < POSITIVE_CLASSIFICATION_THRESHOLD) & (s['ground_truth'] == 1)\n",
    "    tn = (s['Model_predicted_score'] < POSITIVE_CLASSIFICATION_THRESHOLD) & (s['ground_truth'] == 0)\n",
    "    \n",
    "    p.append( tp.sum() / (tp.sum() + fp.sum()))\n",
    "    r.append( tp.sum() / (tp.sum() + fn.sum()))\n",
    "    \n",
    "    auc.append(metrics.roc_auc_score(y_true=s['ground_truth'], y_score=s['Model_predicted_score']))\n",
    "    ap.append(metrics.average_precision_score(y_true=s['ground_truth'], y_score=s['Model_predicted_score']))\n",
    "\n",
    "              \n",
    "perf_stats['Precision'] = p\n",
    "perf_stats['Recall'] = r\n",
    "perf_stats['AUC'] = auc \n",
    "perf_stats['AP'] = ap\n",
    "perf_stats.index = rows \n",
    "              \n",
    "perf_stats.to_latex(f\"{PAPER_GIT_REPO_PATH}/tables/performance_vt.tex\", float_format=\"%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cfe2a7",
   "metadata": {},
   "source": [
    "### Combined AUC / AUPRC Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ffd1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "fig, (axauc, axprc) = plt.subplots(1, 2, figsize=(14,6))\n",
    "#plt.style.use('seaborn-v0_8-paper')\n",
    "RocCurveDisplay.from_predictions(\n",
    "    t.ground_truth,\n",
    "    t.Model_predicted_score,\n",
    "    name=\"Police Vehicle\",\n",
    "    color=\"green\", ax=axauc\n",
    ")\n",
    "axauc.plot([0, 1], [0, 1], \"k--\")\n",
    "axauc.axis(\"square\")\n",
    "axauc.set_xlabel(\"False Positive Rate\", fontsize=20)\n",
    "axauc.set_ylabel(\"True Positive Rate\", fontsize=20)\n",
    "\n",
    "#plt.title(\"ROC Curve\")\n",
    "axauc.legend(prop={'size': 20})\n",
    "#plt.show()\n",
    "\n",
    "axauc.set_xlim(0,1)\n",
    "axauc.set_ylim(0,1)\n",
    "\n",
    "axauc.tick_params(axis='both', which='major', labelsize=14)\n",
    "axauc.tick_params(axis='both', which='minor', labelsize=14)\n",
    "\n",
    "#plt.style.use('seaborn-v0_8-paper')\n",
    "PrecisionRecallDisplay.from_predictions(\n",
    "    t.ground_truth,\n",
    "    t.Model_predicted_score,\n",
    "    name=\"Police Vehicle\",\n",
    "    color=\"purple\", ax=axprc\n",
    ")\n",
    "\n",
    "axauc.set_title(\"AUC\", fontdict = {'fontsize': 24})\n",
    "\n",
    "axprc.axis(\"square\")\n",
    "axprc.set_xlabel(\"Recall\", fontsize=20)\n",
    "axprc.set_ylabel(\"Precision\", fontsize=20)\n",
    "#plt.title(\"ROC Curve\")\n",
    "axprc.legend(prop={'size': 20})\n",
    "#plt.show()\n",
    "\n",
    "axprc.set_xlim(0,1)\n",
    "axprc.set_ylim(0,1)\n",
    "\n",
    "axprc.tick_params(axis='both', which='major', labelsize=14)\n",
    "axprc.tick_params(axis='both', which='minor', labelsize=14)\n",
    "\n",
    "axprc.set_title(\"AUPRC\", fontdict = {'fontsize': 24})\n",
    "\n",
    "plt.savefig(f\"{PAPER_GIT_REPO_PATH}/figures/test_auc_auprc.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b804bdb",
   "metadata": {},
   "source": [
    "### Tuning: Additional Feature on Calibration Plot for In/Out of Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1c61849",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgdf_with_boro = gpd.sjoin(tgdf, nybb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b68b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[\"Manhattan\"] = tgdf_with_boro['BoroName'] == \"Manhattan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c5d16415",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[\"Manhattan\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea477ee3",
   "metadata": {},
   "source": [
    "### Calibration Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef3b95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ground_truth' not in t.columns:\n",
    "    t['ground_truth'] = 0\n",
    "if 'Model_predicted_score' not in t.columns:\n",
    "    t['Model_predicted_score'] = 0\n",
    "else:\n",
    "    # simple correction model for distance from nearest police station. \n",
    "    police_station_model = sm.Logit.from_formula('ground_truth ~ distance_from_nearest_police_station + Model_predicted_score', data=t).fit()\n",
    "    print(police_station_model.summary())\n",
    "    t['police_distance_adjusted_score'] = police_station_model.predict(t)\n",
    "\n",
    "t.loc[t['median_household_income'] == '250,000+', 'median_household_income']  = 250000\n",
    "t.loc[t['median_household_income'] == '2,500-', 'median_household_income']  = 2500\n",
    "\n",
    "t.loc[t['median_household_income'] == '-', 'median_household_income'] = None\n",
    "t['median_household_income'] = t['median_household_income'].astype(float)\n",
    "t.describe()\n",
    "\n",
    "\n",
    "t['classified_positive'] = t.Model_predicted_score > 0.86 # what is our threshold for a positive classification\n",
    "print('p(Police | classified_positive): %2.3f (precision)' % t.loc[t['classified_positive'] == 1, 'ground_truth'].mean())\n",
    "print('p(Police | classified_negative): %2.3f (forget what this metric is called)' % t.loc[t['classified_positive'] == 0, 'ground_truth'].mean())\n",
    "print('p(classified_positive | police): %2.3f (recall)' % t.loc[t['ground_truth'] == 1, 'classified_positive'].mean())\n",
    "\n",
    "\n",
    "results=[]\n",
    "vars_to_plot = ['phase','weekend','daytime', 'percent_white', 'percent_black', 'percent_asian', 'percent_hispanic', 'median_household_income', 'pplpersqmi', 'distance_from_nearest_crime_1hr', 'distance_from_nearest_crime_3hr', 'distance_from_nearest_crime_6hr', 'distance_from_nearest_police_station', 'Manhattan']\n",
    "vars_to_plot_names = ['Phase 1', 'Weekend', 'Daytime', 'Percent White > Median', 'Percent Black > Median', 'Percent Asian > Median', \n",
    "                      'Percent Hispanic > Median', 'Median Household Income > Median', 'Population Density > Median', 'Distance From Nearest Crime [1hr] > Median', 'Distance From Nearest Crime [3hr] > Median', 'Distance From Nearest Crime [6hr] > Median', 'Distance From Nearest Police Station > Median', 'Manhattan']\n",
    "for x in vars_to_plot:\n",
    "    df_to_plot = t.dropna(subset=x).copy()\n",
    "    if df_to_plot[x].median() == (1 | 0): \n",
    "        df_to_plot[\"above_median\"] = df_to_plot[x] == 1\n",
    "    else:\n",
    "        df_to_plot['above_median'] = df_to_plot[x] > df_to_plot[x].median()\n",
    "    \n",
    "    for above_median in [True, False]:\n",
    "        for classified_positive in [True, False]:\n",
    "            idxs = (df_to_plot['above_median'] == above_median) & (df_to_plot['classified_positive'] == classified_positive)\n",
    "            mu = df_to_plot.loc[idxs, 'ground_truth'].mean() # probability of a police car given whether you're above the median and whether you're classified positive. \n",
    "            err = 1.96 * np.sqrt(mu * (1 - mu) / idxs.sum()) # confidence interval: https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval\n",
    "            results.append({'x':x, \n",
    "                            'above_median':above_median, \n",
    "                            'classified_positive':classified_positive,\n",
    "                            'mean':mu, \n",
    "                            'error':err})\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "fig, (axp, axn) = plt.subplots(1,2, figsize=(10,4), sharey='row')\n",
    "\n",
    "\n",
    "\n",
    "# positive classifications plot\n",
    "axp.errorbar(y=range(len(vars_to_plot)),\n",
    "             x=results.loc[(results['classified_positive'] == True) & (results['above_median'] == True), 'mean'], \n",
    "             xerr=results.loc[(results['classified_positive'] == True) & (results['above_median'] == True), 'error'], \n",
    "             label='True', \n",
    "             fmt='.', \n",
    "             markersize=10)\n",
    "axp.errorbar(y=[a + 0.1 for a in range(len(vars_to_plot))],\n",
    "             x=results.loc[(results['classified_positive'] == True) & (results['above_median'] == False), 'mean'], \n",
    "             xerr=results.loc[(results['classified_positive'] == True) & (results['above_median'] == False), 'error'], \n",
    "             label='False', \n",
    "             fmt='.', \n",
    "             markersize=10)\n",
    "#axp.legend(loc='center left')\n",
    "axp.set_yticks(range(len(vars_to_plot)), vars_to_plot_names, fontsize=12)\n",
    "axp.set_xlabel(\"Probability Image Truly has a Police Car\", fontsize=12)\n",
    "axp.set_title(\"Images Classified Positive\", fontsize=12)\n",
    "\n",
    "# negative classifications plot \n",
    "axn.errorbar(y=range(len(vars_to_plot)),\n",
    "             x=results.loc[(results['classified_positive'] == False) & (results['above_median'] == True), 'mean'], \n",
    "             xerr=results.loc[(results['classified_positive'] == False) & (results['above_median'] == True), 'error'], \n",
    "             label='True', \n",
    "             fmt='.', \n",
    "             markersize=10)\n",
    "axn.errorbar(y=[a + 0.1 for a in range(len(vars_to_plot))],\n",
    "             x=results.loc[(results['classified_positive'] == False) & (results['above_median'] == False), 'mean'], \n",
    "             xerr=results.loc[(results['classified_positive'] == False) & (results['above_median'] == False), 'error'], \n",
    "             label='False', \n",
    "             fmt='.', \n",
    "             markersize=10)\n",
    "\n",
    "\n",
    "axn.legend(loc='center right')\n",
    "#axn.set_yticks(range(len(vars_to_plot)), '')\n",
    "axn.set_xlabel(\"Probability Image Truly has a Police Car\", fontsize=12)\n",
    "axn.set_title(\"Images Classified Negative\", fontsize=12)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{PAPER_GIT_REPO_PATH}/figures/calplots.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc406b",
   "metadata": {},
   "source": [
    "### AUC/ AUPRC By Subgroups Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18c97a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# AUC/AUPRC by subgroup as well. \n",
    "auc_auprc_results = []\n",
    "for x in vars_to_plot:\n",
    "    df_to_plot = t.dropna(subset=x).copy()\n",
    "\n",
    "    if df_to_plot[x].median() == (1 | 0): \n",
    "        df_to_plot[\"above_median\"] = df_to_plot[x] == 1\n",
    "    else:\n",
    "        df_to_plot['above_median'] = df_to_plot[x] > df_to_plot[x].median()\n",
    "    \n",
    "    for above_median in [True, False]:\n",
    "        \n",
    "        auc = metrics.roc_auc_score(y_true=df_to_plot.loc[df_to_plot['above_median'] == above_median,'ground_truth'], \n",
    "                                    y_score=df_to_plot.loc[df_to_plot['above_median'] == above_median,'Model_predicted_score'])\n",
    "\n",
    "        average_precision = metrics.average_precision_score(\n",
    "            y_true=df_to_plot.loc[df_to_plot['above_median'] == above_median,'ground_truth'], \n",
    "            y_score=df_to_plot.loc[df_to_plot['above_median'] == above_median,'Model_predicted_score'])\n",
    "        auc_auprc_results.append({'x':x, \n",
    "                                  'above_median':above_median, \n",
    "                                  'auc':auc, \n",
    "                                  'auprc':average_precision, \n",
    "                                  'n':(df_to_plot['above_median'] == above_median).sum(), \n",
    "                                  'n_pos':df_to_plot.loc[df_to_plot['above_median'] == above_median,'ground_truth'].sum()})\n",
    "\n",
    "#pd.DataFrame(auc_auprc_results).to_csv(f\"valset_{os.path.splitext(FILENAME)[0][-1]}_auc_by_subgroup.csv\",index=False)\n",
    "\n",
    "auc_auprc_results_table = pd.DataFrame(auc_auprc_results)\n",
    "\n",
    "rename = {'x': 'Subgroup', 'above_median': 'Above Median?', 'auc': 'AUC', 'auprc': 'Average Precision'}\n",
    "\n",
    "auc_auprc_results_table.rename(columns=rename, inplace=True)\n",
    "auc_auprc_results_table.drop(['n','n_pos'], axis=1, inplace=True)\n",
    "\n",
    "auc_auprc_results_table = auc_auprc_results_table.groupby(['Subgroup', 'Above Median?'], sort=False).sum().unstack(level=1)\n",
    "\n",
    "subgroups_print = {'day_of_month': 'Day of Month', 'daytime': 'Daytime', 'distance_from_nearest_crime_1hr': 'Distance From Nearest Crime [1hr] > Median', 'distance_from_nearest_crime_3hr': 'Distance From Nearest Crime [3hr] > Median', 'distance_from_nearest_crime_6hr': 'Distance From Nearest Crime [6hr] > Median', 'distance_from_nearest_police_station': 'Distance From Nearest Police Station > Median', 'median_household_income': 'Median Household Income > Median', 'month': 'Month', 'percent_black': 'Percent Black > Median', 'percent_white': 'Percent White > Median', 'percent_hispanic': 'Percent Hispanic > Median', 'percent_asian': 'Percent Asian > Median', 'phase': 'Phase 1', 'pplpersqmi': 'Population Density > Median', 'weekend': 'Weekend', 'Manhattan': 'Manhattan'}\n",
    "vars_to_plot_names = ['Phase 1', 'Weekend', 'Daytime', 'Percent White > Median', 'Percent Black > Median', 'Percent Asian > Median', \n",
    "                      'Percent Hispanic > Median', 'Median Household Income > Median', 'Population Density > Median', 'Distance From Nearest Crime [1hr] > Median', 'Distance From Nearest Crime [3hr] > Median', 'Distance From Nearest Crime [6hr] > Median', 'Distance From Nearest Police Station > Median']\n",
    "auc_auprc_results_table.index = auc_auprc_results_table.index.map(subgroups_print)\n",
    "auc_aurpc_results_table = auc_auprc_results_table.iloc[::-1]\n",
    "auc_auprc_results_table.to_latex(f'{PAPER_GIT_REPO_PATH}/tables/auc_auprc_results_by_subgroup.tex', float_format=\"%.2f\") \n",
    "\n",
    "auc_aurpc_results_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fpp_geospatial] *",
   "language": "python",
   "name": "conda-env-fpp_geospatial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
