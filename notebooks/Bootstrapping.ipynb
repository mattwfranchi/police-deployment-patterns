{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd724a9",
   "metadata": {},
   "source": [
    "# Bootstrapping Results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a3cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import json\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c76bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALSET_PATH = '/share/pierson/nexar_data/dashcam-analysis/final_model_metrics/valset_2.csv'\n",
    "BASE_CHUNKS_PATH = '/share/pierson/nexar_data/FINAL_CHUNKS_ETHNICITY_DATA/%i.csv'\n",
    "N_CHUNKS = 20\n",
    "COLS_TO_DEDUPLICATE_ON = ['lat', 'lng', 'timestamp'] # columns to use to check for duplicates\n",
    "MIN_DATE_FOR_DEMOGRAPHIC_ANALYSIS = datetime.datetime(2020, 10, 5) # don't use data before this data to analyze disparities / demographics\n",
    "POSITIVE_CLASSIFICATION_THRESHOLD = 0.77 # threshold to define a positive prediction\n",
    "LOCATION_COL_TO_GROUP_ON = 'NAME' # This should be the name of the column we're analyzing location grouping at - e.g., corresponding to Census Block Group or Census tract.\n",
    "TOTAL_POPULATION_COL = 'Estimate_Total' # needs to match whether using Census tract or Block group. \n",
    "WHITE_POPULATION_COL = 'Estimate_Total_Not_Hispanic_or_Latino_White_alone'\n",
    "BLACK_POPULATION_COL = 'Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone'\n",
    "HISPANIC_POPULATION_COL = 'Estimate_Total_Hispanic_or_Latino'\n",
    "ASIAN_POPULATION_COL = 'Estimate_Total_Not_Hispanic_or_Latino_Asian_alone'\n",
    "POPULATION_COUNT_COLS = [WHITE_POPULATION_COL, BLACK_POPULATION_COL, HISPANIC_POPULATION_COL, ASIAN_POPULATION_COL, TOTAL_POPULATION_COL]\n",
    "TIME_AND_DATE_COL = 'time_and_date_of_image'\n",
    "DEMOGRAPHIC_COLS = ['density_cbg', # things we want to look at correlations with. Demographic cols may not be best name. \n",
    "                    'black_frac',\n",
    "                    'white_frac', \n",
    "                    'distance_from_nearest_crime_6hr',\n",
    "                    'distance_from_nearest_police_station',\n",
    "                    'median_household_income']\n",
    "PREDICTION_COLS = ['above_threshold', 'calibrated_prediction', 'prediction_adjusted_for_police_station_distance'] # columns with police car predictions. We define these\n",
    "MIN_POPULATION_IN_AREA = 500\n",
    "BOROUGH_COL = 'boroname'\n",
    "NEIGHBORHOOD_COL = 'ntaname'\n",
    "N_BOOTSTRAPS = 20\n",
    "ZONE_THRESHOLD = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da7adb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in chunk 0 with 1115281 rows\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_CHUNKS):\n\u001b[0;32m----> 3\u001b[0m     d_i \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_CHUNKS_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRead in chunk \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (i, \u001b[38;5;28mlen\u001b[39m(d_i)))\n\u001b[1;32m      5\u001b[0m     d\u001b[38;5;241m.\u001b[39mappend(d_i)\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/share/pierson/conda_virtualenvs/fpp_geospatial/lib/python3.11/site-packages/pandas/core/dtypes/common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1430\u001b[0m     )\n\u001b[0;32m-> 1433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = []\n",
    "for i in range(N_CHUNKS):\n",
    "    d_i = pd.read_csv(BASE_CHUNKS_PATH % i)\n",
    "    print('Read in chunk %i with %i rows' % (i, len(d_i)))\n",
    "    d.append(d_i)\n",
    "d = pd.concat(d)\n",
    "#d.iloc[0][[a for a in d.columns if 'Margin of Error' not in a and 'Two races' not in a]] # just print out what dataframe looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab0e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates. \n",
    "duplicate_idxs = d.duplicated(subset=COLS_TO_DEDUPLICATE_ON)\n",
    "print(\"warning: %i duplicates identified using %s, fraction %2.6f of rows; dropping rows\" % (duplicate_idxs.sum(), COLS_TO_DEDUPLICATE_ON, duplicate_idxs.mean()))\n",
    "d = d.loc[~duplicate_idxs].copy()\n",
    "\n",
    "cbg_zone_data = pd.read_csv('/share/pierson/nexar_data/5_other_datasets/cbgs_zone_data.csv')\n",
    "assert (1.*(cbg_zone_data['C'] > ZONE_THRESHOLD) + 1.*(cbg_zone_data['M'] > ZONE_THRESHOLD) + 1.*(cbg_zone_data['R'] > ZONE_THRESHOLD)).max() == 1\n",
    "cbg_zone_dict = {}\n",
    "for zone_val in ['C', 'M', 'R']:\n",
    "    zones = cbg_zone_data.loc[cbg_zone_data[zone_val] >= ZONE_THRESHOLD]\n",
    "    print(\"%i CBGs classified as %s\" % (len(zones), zone_val))\n",
    "    cbg_zone_dict.update(dict(zip(zones['GEOID20'].values, [zone_val for _ in range(len(zones))])))\n",
    "print(len(cbg_zone_dict))\n",
    "d['zone'] = d['GEOID20'].map(lambda x:cbg_zone_dict[x] if x in cbg_zone_dict else None)\n",
    "print(\"zone classification of images\")\n",
    "print(d['zone'].value_counts(dropna=False))\n",
    "\n",
    "def household_income_map(x):\n",
    "    if x == '-':\n",
    "        return None\n",
    "    elif x == '250,000+':\n",
    "        return 250000\n",
    "    elif x == '2,500-':\n",
    "        return 2500\n",
    "    return float(x)\n",
    "\n",
    "# define Census variables\n",
    "d['median_household_income'] = d['median_household_income'].map(household_income_map)\n",
    "d['white_frac'] = d[WHITE_POPULATION_COL] / d[TOTAL_POPULATION_COL]\n",
    "d['black_frac'] = d[BLACK_POPULATION_COL] / d[TOTAL_POPULATION_COL]\n",
    "assert d['white_frac'].dropna().max() <= 1\n",
    "assert d['white_frac'].dropna().min() >= 0\n",
    "assert d['black_frac'].dropna().max() <= 1\n",
    "assert d['black_frac'].dropna().min() >= 0\n",
    "\n",
    "\n",
    "# define time variables\n",
    "d['date'] = d[TIME_AND_DATE_COL].map(lambda x:datetime.datetime.strptime(x.split()[0], '%Y-%m-%d'))\n",
    "locations_by_date = d.groupby('date')[LOCATION_COL_TO_GROUP_ON].nunique()\n",
    "print('unique locations by', locations_by_date)\n",
    "\n",
    "# filter for dates with full coverage. \n",
    "print(\"In demographic analysis, filtering for locations after %s because more geographically representative\" % MIN_DATE_FOR_DEMOGRAPHIC_ANALYSIS)\n",
    "d_for_demo_analysis = d.loc[d['date'] >= MIN_DATE_FOR_DEMOGRAPHIC_ANALYSIS].copy()\n",
    "print(\"%i/%i rows remaining\" % (len(d_for_demo_analysis), len(d)))\n",
    "\n",
    "for col in [WHITE_POPULATION_COL, BLACK_POPULATION_COL, HISPANIC_POPULATION_COL, ASIAN_POPULATION_COL, TOTAL_POPULATION_COL]:\n",
    "    print(\"Setting fraction %2.6f of rows with %s = NA to 0\" % (d_for_demo_analysis[col].isnull().mean(), \n",
    "                                                            col))\n",
    "    d_for_demo_analysis.loc[d_for_demo_analysis[col].isnull(), col] = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fpp_geospatial] *",
   "language": "python",
   "name": "conda-env-fpp_geospatial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
