{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e77872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25188ab5",
   "metadata": {},
   "source": [
    "Todos: \n",
    "\n",
    "1. improve filtering? (Not sure what stuff we should be filtering out.) Improve duplicate checks? \n",
    "2. Make sure all the column names are correct. In particular, I'm not sure if I'm using the right location grouping column, and we should be using consistent tract/block group fields. For example, is median_household_income at tract or Block group level? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5871c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALSET_PATH = '/share/pierson/nexar_data/dashcam-analysis/final_model_metrics/valset_2.csv'\n",
    "BASE_CHUNKS_PATH = '/share/pierson/nexar_data/FINAL_CHUNKS_ETHNICITY_DATA/%i.csv'\n",
    "N_CHUNKS = 20\n",
    "COLS_TO_DEDUPLICATE_ON = ['lat', 'lng', 'timestamp'] # columns to use to check for duplicates\n",
    "MIN_DATE_FOR_DEMOGRAPHIC_ANALYSIS = datetime.datetime(2020, 10, 5) # don't use data before this data to analyze disparities / demographics\n",
    "POSITIVE_CLASSIFICATION_THRESHOLD = 0.77 # threshold to define a positive prediction\n",
    "LOCATION_COL_TO_GROUP_ON = 'NAME' # This should be the name of the column we're analyzing location grouping at - e.g., corresponding to Census Block Group or Census tract.\n",
    "TOTAL_POPULATION_COL = 'Estimate_Total' # needs to match whether using Census tract or Block group. \n",
    "WHITE_POPULATION_COL = 'Estimate_Total_Not_Hispanic_or_Latino_White_alone'\n",
    "BLACK_POPULATION_COL = 'Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone'\n",
    "HISPANIC_POPULATION_COL = 'Estimate_Total_Hispanic_or_Latino'\n",
    "ASIAN_POPULATION_COL = 'Estimate_Total_Not_Hispanic_or_Latino_Asian_alone'\n",
    "POPULATION_COUNT_COLS = [WHITE_POPULATION_COL, BLACK_POPULATION_COL, HISPANIC_POPULATION_COL, ASIAN_POPULATION_COL, TOTAL_POPULATION_COL]\n",
    "TIME_AND_DATE_COL = 'time_and_date_of_image'\n",
    "DEMOGRAPHIC_COLS = ['density_cbg', # things we want to look at correlations with. Demographic cols may not be best name. \n",
    "                    'black_frac',\n",
    "                    'white_frac', \n",
    "                    'distance_from_nearest_crime_6hr',\n",
    "                    'distance_from_nearest_police_station',\n",
    "                    'median_household_income']\n",
    "PREDICTION_COLS = ['above_threshold', 'calibrated_prediction', 'prediction_adjusted_for_police_station_distance'] # columns with police car predictions. We define these\n",
    "MIN_POPULATION_IN_AREA = 500\n",
    "BOROUGH_COL = 'boroname'\n",
    "NEIGHBORHOOD_COL = 'ntaname'\n",
    "N_BOOTSTRAPS = 20\n",
    "ZONE_THRESHOLD = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fac977",
   "metadata": {},
   "source": [
    "# read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f96c7d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in chunk 0 with 1115281 rows\n",
      "Read in chunk 1 with 1115281 rows\n",
      "Read in chunk 2 with 1115281 rows\n",
      "Read in chunk 3 with 1115281 rows\n",
      "Read in chunk 4 with 1115281 rows\n",
      "Read in chunk 5 with 1115281 rows\n",
      "Read in chunk 6 with 1115281 rows\n",
      "Read in chunk 7 with 1115281 rows\n",
      "Read in chunk 8 with 1115281 rows\n",
      "Read in chunk 9 with 1115281 rows\n",
      "Read in chunk 10 with 1115281 rows\n",
      "Read in chunk 11 with 1115281 rows\n",
      "Read in chunk 12 with 1115281 rows\n",
      "Read in chunk 13 with 1115281 rows\n",
      "Read in chunk 14 with 1115280 rows\n",
      "Read in chunk 15 with 1115280 rows\n",
      "Read in chunk 16 with 1115280 rows\n",
      "Read in chunk 17 with 1115280 rows\n",
      "Read in chunk 18 with 1115280 rows\n",
      "Read in chunk 19 with 1115280 rows\n"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "for i in range(N_CHUNKS):\n",
    "    d_i = pd.read_csv(BASE_CHUNKS_PATH % i)\n",
    "    print('Read in chunk %i with %i rows' % (i, len(d_i)))\n",
    "    d.append(d_i)\n",
    "d = pd.concat(d)\n",
    "#d.iloc[0][[a for a in d.columns if 'Margin of Error' not in a and 'Two races' not in a]] # just print out what dataframe looks like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82859c",
   "metadata": {},
   "source": [
    "# apply filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b44e2ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: 203 duplicates identified using ['lat', 'lng', 'timestamp'], fraction 0.000009 of rows; dropping rows\n",
      "383 CBGs classified as C\n",
      "276 CBGs classified as M\n",
      "5907 CBGs classified as R\n",
      "6566\n",
      "zone classification of images\n",
      "R      15737220\n",
      "C       3421338\n",
      "M       2542381\n",
      "NaN      604472\n",
      "Name: zone, dtype: int64\n",
      "unique locations by date\n",
      "2020-03-05    1590\n",
      "2020-03-06    1441\n",
      "2020-03-13    1511\n",
      "2020-03-20    1221\n",
      "2020-03-27     728\n",
      "2020-04-03     497\n",
      "2020-04-10     555\n",
      "2020-04-17     561\n",
      "2020-04-24     875\n",
      "2020-05-01     639\n",
      "2020-05-08     735\n",
      "2020-05-15     854\n",
      "2020-05-22     956\n",
      "2020-05-29     924\n",
      "2020-06-05     662\n",
      "2020-06-12     625\n",
      "2020-06-19    1106\n",
      "2020-06-26    1094\n",
      "2020-07-03    1142\n",
      "2020-07-26     128\n",
      "2020-07-27     299\n",
      "2020-07-31    1290\n",
      "2020-08-01     326\n",
      "2020-08-02     268\n",
      "2020-08-03     359\n",
      "2020-08-07    1384\n",
      "2020-08-08     476\n",
      "2020-08-09     460\n",
      "2020-08-10     298\n",
      "2020-08-14    1334\n",
      "2020-08-15     538\n",
      "2020-08-16     409\n",
      "2020-08-17     405\n",
      "2020-08-21    1311\n",
      "2020-08-22     718\n",
      "2020-08-23     746\n",
      "2020-08-24     516\n",
      "2020-08-28    1377\n",
      "2020-08-29     722\n",
      "2020-08-30     590\n",
      "2020-08-31     699\n",
      "2020-10-05    6517\n",
      "2020-10-06    6536\n",
      "2020-10-07    6536\n",
      "2020-10-08    6530\n",
      "2020-10-09    6538\n",
      "2020-10-10    6547\n",
      "2020-10-11    6534\n",
      "2020-10-12    6528\n",
      "2020-10-13    6513\n",
      "2020-10-14    6526\n",
      "2020-10-15    6548\n",
      "2020-10-16    6544\n",
      "2020-10-17    6552\n",
      "2020-10-18    6538\n",
      "2020-10-19    6517\n",
      "2020-10-20    6531\n",
      "2020-10-21    6532\n",
      "2020-10-22    6521\n",
      "2020-10-23    6523\n",
      "2020-10-24    6540\n",
      "2020-10-25    6524\n",
      "2020-10-26    6531\n",
      "2020-10-27    6522\n",
      "2020-10-28    6510\n",
      "2020-10-29    6533\n",
      "2020-10-30    6539\n",
      "2020-10-31    6549\n",
      "2020-11-01    6522\n",
      "2020-11-02    6517\n",
      "2020-11-03    6518\n",
      "2020-11-04    6507\n",
      "2020-11-05    6523\n",
      "2020-11-06    6525\n",
      "2020-11-07    6536\n",
      "2020-11-08    6525\n",
      "2020-11-09    6519\n",
      "2020-11-10    6523\n",
      "2020-11-11    6497\n",
      "2020-11-12    6526\n",
      "2020-11-13    6530\n",
      "2020-11-14    6530\n",
      "2020-11-15    6503\n",
      "2020-11-16    5940\n",
      "Name: NAME, dtype: int64\n",
      "In demographic analysis, filtering for locations after 2020-10-05 00:00:00 because more geographically representative\n",
      "18719816/22305411 rows remaining\n",
      "Setting fraction 0.000229 of rows with Estimate_Total_Not_Hispanic_or_Latino_White_alone = NA to 0\n",
      "Setting fraction 0.000229 of rows with Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone = NA to 0\n",
      "Setting fraction 0.000229 of rows with Estimate_Total_Hispanic_or_Latino = NA to 0\n",
      "Setting fraction 0.000229 of rows with Estimate_Total_Not_Hispanic_or_Latino_Asian_alone = NA to 0\n",
      "Setting fraction 0.000229 of rows with Estimate_Total = NA to 0\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates. \n",
    "duplicate_idxs = d.duplicated(subset=COLS_TO_DEDUPLICATE_ON)\n",
    "print(\"warning: %i duplicates identified using %s, fraction %2.6f of rows; dropping rows\" % (duplicate_idxs.sum(), COLS_TO_DEDUPLICATE_ON, duplicate_idxs.mean()))\n",
    "d = d.loc[~duplicate_idxs].copy()\n",
    "\n",
    "cbg_zone_data = pd.read_csv('/share/pierson/nexar_data/5_other_datasets/cbgs_zone_data.csv')\n",
    "assert (1.*(cbg_zone_data['C'] > ZONE_THRESHOLD) + 1.*(cbg_zone_data['M'] > ZONE_THRESHOLD) + 1.*(cbg_zone_data['R'] > ZONE_THRESHOLD)).max() == 1\n",
    "cbg_zone_dict = {}\n",
    "for zone_val in ['C', 'M', 'R']:\n",
    "    zones = cbg_zone_data.loc[cbg_zone_data[zone_val] >= ZONE_THRESHOLD]\n",
    "    print(\"%i CBGs classified as %s\" % (len(zones), zone_val))\n",
    "    cbg_zone_dict.update(dict(zip(zones['GEOID20'].values, [zone_val for _ in range(len(zones))])))\n",
    "print(len(cbg_zone_dict))\n",
    "d['zone'] = d['GEOID20'].map(lambda x:cbg_zone_dict[x] if x in cbg_zone_dict else None)\n",
    "print(\"zone classification of images\")\n",
    "print(d['zone'].value_counts(dropna=False))\n",
    "\n",
    "def household_income_map(x):\n",
    "    if x == '-':\n",
    "        return None\n",
    "    elif x == '250,000+':\n",
    "        return 250000\n",
    "    elif x == '2,500-':\n",
    "        return 2500\n",
    "    return float(x)\n",
    "\n",
    "# define Census variables\n",
    "d['median_household_income'] = d['median_household_income'].map(household_income_map)\n",
    "d['white_frac'] = d[WHITE_POPULATION_COL] / d[TOTAL_POPULATION_COL]\n",
    "d['black_frac'] = d[BLACK_POPULATION_COL] / d[TOTAL_POPULATION_COL]\n",
    "assert d['white_frac'].dropna().max() <= 1\n",
    "assert d['white_frac'].dropna().min() >= 0\n",
    "assert d['black_frac'].dropna().max() <= 1\n",
    "assert d['black_frac'].dropna().min() >= 0\n",
    "\n",
    "\n",
    "# define time variables\n",
    "d['date'] = d[TIME_AND_DATE_COL].map(lambda x:datetime.datetime.strptime(x.split()[0], '%Y-%m-%d'))\n",
    "locations_by_date = d.groupby('date')[LOCATION_COL_TO_GROUP_ON].nunique()\n",
    "print('unique locations by', locations_by_date)\n",
    "\n",
    "# filter for dates with full coverage. \n",
    "print(\"In demographic analysis, filtering for locations after %s because more geographically representative\" % MIN_DATE_FOR_DEMOGRAPHIC_ANALYSIS)\n",
    "d_for_demo_analysis = d.loc[d['date'] >= MIN_DATE_FOR_DEMOGRAPHIC_ANALYSIS].copy()\n",
    "print(\"%i/%i rows remaining\" % (len(d_for_demo_analysis), len(d)))\n",
    "\n",
    "for col in [WHITE_POPULATION_COL, BLACK_POPULATION_COL, HISPANIC_POPULATION_COL, ASIAN_POPULATION_COL, TOTAL_POPULATION_COL]:\n",
    "    print(\"Setting fraction %2.6f of rows with %s = NA to 0\" % (d_for_demo_analysis[col].isnull().mean(), \n",
    "                                                            col))\n",
    "    d_for_demo_analysis.loc[d_for_demo_analysis[col].isnull(), col] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49996b",
   "metadata": {},
   "source": [
    "# compute probability measures using validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1ff5b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of val set classified positive: 0.013 (265 rows)\n",
      "Pr(true positive | classified positive): 0.777\n",
      "Pr(true positive | classified negative): 0.002\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.017459\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:           ground_truth   No. Observations:                20000\n",
      "Model:                          Logit   Df Residuals:                    19997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Sun, 05 Feb 2023   Pseudo R-squ.:                  0.7376\n",
      "Time:                        15:15:46   Log-Likelihood:                -349.18\n",
      "converged:                       True   LL-Null:                       -1330.8\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "========================================================================================================\n",
      "                                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Intercept                               -6.7151      0.289    -23.230      0.000      -7.282      -6.149\n",
      "Model_predicted_score                    8.9204      0.313     28.518      0.000       8.307       9.533\n",
      "distance_from_nearest_police_station    -0.0001   6.05e-05     -2.346      0.019      -0.000   -2.34e-05\n",
      "========================================================================================================\n",
      "Mean value of prediction column above_threshold: 0.013; std 0.114; > 0 0.013\n",
      "Mean value of prediction column calibrated_prediction: 0.012; std 0.088; > 0 1.000\n",
      "Mean value of prediction column prediction_adjusted_for_police_station_distance: 0.012; std 0.086; > 0 1.000\n"
     ]
    }
   ],
   "source": [
    "def calibrate_probabilities_using_valset(val_d, d_to_add_prediction_columns_to):\n",
    "    \"\"\"\n",
    "    Annotate a dataframe, d_to_add_prediction_columns_to, with three prediction columns\n",
    "    derived from the val set val_d.\n",
    "    \n",
    "    1. A simple binary variable with whether conf > POSITIVE_CLASSIFICATION_THRESHOLD\n",
    "    2. A probabilistic prediction from val set: if above threshold, Pr(ground truth positive | above threshold in val set)\n",
    "    and if below threshold, Pr(ground truth negative | below threshold in val set)\n",
    "    3. A probability adjusted for police station distance. Not sure if this is a good thing to use, and should definitely check it is calibrated on test set if we do.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. annotate with simple binary score\n",
    "    assert val_d['Model_predicted_score'].isnull().sum() == 0\n",
    "    val_d['classified_positive'] = val_d['Model_predicted_score'] > POSITIVE_CLASSIFICATION_THRESHOLD\n",
    "    d_to_add_prediction_columns_to['above_threshold'] = (d_to_add_prediction_columns_to['conf'] > POSITIVE_CLASSIFICATION_THRESHOLD) * 1.\n",
    "    \n",
    "    # 2. compute probabilities given above/below threshold from val set\n",
    "    p_positive_given_classified_positive = val_d.loc[val_d['classified_positive'] == True, 'ground_truth'].mean()\n",
    "    p_positive_given_classified_negative = val_d.loc[val_d['classified_positive'] == False, 'ground_truth'].mean()\n",
    "    print(\"Fraction of val set classified positive: %2.3f (%i rows)\" % \n",
    "          (val_d['classified_positive'].mean(), val_d['classified_positive'].sum()))\n",
    "    print(\"Pr(true positive | classified positive): %2.3f\" % p_positive_given_classified_positive)\n",
    "    print(\"Pr(true positive | classified negative): %2.3f\" % p_positive_given_classified_negative)\n",
    "    d_to_add_prediction_columns_to['calibrated_prediction'] = d_to_add_prediction_columns_to['above_threshold'].map(lambda x:p_positive_given_classified_positive if x == 1 else p_positive_given_classified_negative) \n",
    "    \n",
    "    # 3. compute adjusted probability given police station distance. Not sure if this is necessary or wise, but adding just in case. \n",
    "    police_station_distance_model = sm.Logit.from_formula('ground_truth ~ Model_predicted_score + distance_from_nearest_police_station', data=val_d).fit()\n",
    "    print(police_station_distance_model.summary())\n",
    "    d_to_add_prediction_columns_to['Model_predicted_score'] = 0 # compute police-distance adjusted probability on d_to_add_prediction_columns_to. \n",
    "    d_to_add_prediction_columns_to.loc[~pd.isnull(d_to_add_prediction_columns_to['conf']), 'Model_predicted_score'] = d_to_add_prediction_columns_to['conf'].loc[~pd.isnull(d_to_add_prediction_columns_to['conf'])]\n",
    "    assert d_to_add_prediction_columns_to['Model_predicted_score'].isnull().sum() == 0\n",
    "    d_to_add_prediction_columns_to['prediction_adjusted_for_police_station_distance'] = police_station_distance_model.predict(d_to_add_prediction_columns_to).values\n",
    "    del d_to_add_prediction_columns_to['Model_predicted_score']\n",
    "    \n",
    "    added_cols = ['above_threshold', 'calibrated_prediction', 'prediction_adjusted_for_police_station_distance']\n",
    "    assert pd.isnull(d_to_add_prediction_columns_to[added_cols]).values.sum() == 0\n",
    "    assert (d_to_add_prediction_columns_to[added_cols].values < 0).sum() == 0\n",
    "    assert (d_to_add_prediction_columns_to[added_cols].values > 1).sum() == 0\n",
    "    for col in added_cols:\n",
    "        print(\"Mean value of prediction column %s: %2.3f; std %2.3f; > 0 %2.3f\" % (\n",
    "            col,\n",
    "            d_to_add_prediction_columns_to[col].mean(), \n",
    "            d_to_add_prediction_columns_to[col].std(), \n",
    "            (d_to_add_prediction_columns_to[col] > 0).mean()))\n",
    "    \n",
    "    return d_to_add_prediction_columns_to\n",
    "    \n",
    "val_d = pd.read_csv(VALSET_PATH)\n",
    "d_for_demo_analysis = calibrate_probabilities_using_valset(val_d=val_d, d_to_add_prediction_columns_to=d_for_demo_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f2e12",
   "metadata": {},
   "source": [
    "# disparities analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0da2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6602 unique Census areas using column NAME\n",
      "Population statistics by area\n",
      "count    6602.000000\n",
      "mean     1269.244471\n",
      "std       641.995903\n",
      "min         0.000000\n",
      "1%          0.000000\n",
      "5%        357.000000\n",
      "10%       579.000000\n",
      "50%      1192.000000\n",
      "90%      2090.000000\n",
      "99%      3104.960000\n",
      "max      8541.000000\n",
      "Name: Estimate_Total, dtype: float64\n",
      "excluding census areas with population < 500 keeps fraction 0.989 of population\n",
      "summed values of Estimate_Total_Not_Hispanic_or_Latino_White_alone: 2676732\n",
      "summed values of Estimate_Total_Not_Hispanic_or_Latino_Black_or_African_American_alone: 1795055\n",
      "summed values of Estimate_Total_Hispanic_or_Latino: 2423869\n",
      "summed values of Estimate_Total_Not_Hispanic_or_Latino_Asian_alone: 1185423\n",
      "summed values of Estimate_Total: 8379552\n"
     ]
    }
   ],
   "source": [
    "# group by Census area. \n",
    "grouped_d = d_for_demo_analysis.groupby(LOCATION_COL_TO_GROUP_ON)[POPULATION_COUNT_COLS + \n",
    "                                                              DEMOGRAPHIC_COLS + \n",
    "                                                              PREDICTION_COLS].agg('mean')\n",
    "for col in POPULATION_COUNT_COLS + DEMOGRAPHIC_COLS: # check consistent values by location for demographics. Should only be one value of population count per Census area, for example. \n",
    "    if col in ['distance_from_nearest_crime_6hr', 'distance_from_nearest_police_station']:\n",
    "        continue\n",
    "    assert d_for_demo_analysis.groupby(LOCATION_COL_TO_GROUP_ON)[col].nunique().map(lambda x:x in [0, 1]).all()\n",
    "\n",
    "print(\"%i unique Census areas using column %s\" % (len(grouped_d), LOCATION_COL_TO_GROUP_ON))\n",
    "print(\"Population statistics by area\")\n",
    "print(grouped_d[TOTAL_POPULATION_COL].describe([0.01, 0.05, 0.1, 0.5, 0.9, 0.99]))\n",
    "print(\"excluding census areas with population < %i keeps fraction %2.3f of population\" % \n",
    "      (MIN_POPULATION_IN_AREA, \n",
    "       grouped_d.loc[grouped_d[TOTAL_POPULATION_COL] >= MIN_POPULATION_IN_AREA, TOTAL_POPULATION_COL].sum()/grouped_d[TOTAL_POPULATION_COL].sum()))\n",
    "for col in POPULATION_COUNT_COLS: # sanity check that total counts look right. \n",
    "    print(\"summed values of %s: %i\" % (col, grouped_d[col].sum()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3c43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_disparities_estimator(df, census_area_col, weighting_cols, total_population_col, estimate_col, check_consistent_vals_by_group):\n",
    "    \"\"\"\n",
    "    Given a census dataframe\n",
    "    group by census_area_col and compute the mean value of estimate_col in each census area\n",
    "    then return weighted means across Census areas, \n",
    "    weighting each Census area by all the columns in weighting_cols and total_population_col. \n",
    "    We use this for computations of race-specific results. \n",
    "    Emma reviewed. \n",
    "    \"\"\"\n",
    "    grouped_d = df.groupby(census_area_col)[weighting_cols + [total_population_col, estimate_col]].mean().reset_index()\n",
    "    if check_consistent_vals_by_group: # sanity check to make sure that values are consistent\n",
    "        consistency_df = df.groupby(census_area_col)[weighting_cols + [total_population_col]].nunique()\n",
    "        assert (consistency_df.values == 1).all()\n",
    "        assert df[weighting_cols + [total_population_col, estimate_col]].isnull().values.sum() == 0\n",
    "    results = {}\n",
    "    for col in weighting_cols + [total_population_col]:\n",
    "        results['%s_weighted_mean' % col] = (grouped_d[estimate_col] * grouped_d[col]).sum()/grouped_d[col].sum()\n",
    "    for col in weighting_cols:\n",
    "        results['%s_relative_to_average' % col] = results['%s_weighted_mean' % col]/results['%s_weighted_mean' % total_population_col]\n",
    "    return results\n",
    "\n",
    "def weighted_disparities_estimator_two_level_grouping(df, census_area_col, high_level_group_col, total_population_col, estimate_col, check_consistent_vals_by_group):\n",
    "    \"\"\"\n",
    "    This function is similar to that above, but is (hopefully) a faster way to compute \n",
    "    two-level groupings (e.g., we want to compute borough-specific numbers, and weight by Census tract population within borough). \n",
    "    high_level_group col specifies the column we want to compute disparities over (e.g. borough). \n",
    "    All other columns are as explained above. \n",
    "    Verified that this gives identical results to function above for borough, zone, etc. \n",
    "    \"\"\"\n",
    "    if check_consistent_vals_by_group: # sanity check to make sure that values are consistent\n",
    "        consistency_df = df.groupby(census_area_col)[high_level_group_col].nunique()\n",
    "        assert ((consistency_df.values == 1) | (consistency_df.values == 0)).all()\n",
    "    results = {}\n",
    "    # first compute overall mean. \n",
    "    overall_mean_grouping = df.groupby(census_area_col)[[total_population_col, estimate_col]].mean().reset_index()\n",
    "    results['%s_weighted_mean' % total_population_col] = (overall_mean_grouping[estimate_col] * overall_mean_grouping[total_population_col]).sum()/overall_mean_grouping[total_population_col].sum()\n",
    "    high_level_grouping = df.groupby(high_level_group_col)\n",
    "    all_names = []\n",
    "    for name, group_df in high_level_grouping:\n",
    "        if group_df[total_population_col].sum() == 0:\n",
    "            print(\"Skipping %s because total population is 0\" % name)\n",
    "            continue\n",
    "        all_names.append(name)\n",
    "        second_level_grouping = group_df.groupby(census_area_col)[[total_population_col, estimate_col]].mean().reset_index()\n",
    "        results['%s_weighted_mean' % name] = (second_level_grouping[estimate_col] * second_level_grouping[total_population_col]).sum()/second_level_grouping[total_population_col].sum()\n",
    "    for name in all_names:\n",
    "        results['%s_relative_to_average' % name] = results['%s_weighted_mean' % name]/results['%s_weighted_mean' % total_population_col]\n",
    "    return results\n",
    "\n",
    "def bootstrap_function_errorbars(df, fxn_to_apply, fxn_kwargs, n_bootstraps=100, filename=None):\n",
    "    # compute the point estimate fxn_to_apply(df) on the original data\n",
    "    # and then do bootstrap iterates. Emma reviewed. \n",
    "    bootstrap_statistics = []\n",
    "    point_estimate = fxn_to_apply(df, check_consistent_vals_by_group=True, **fxn_kwargs)\n",
    "    for bootstrap in tqdm(range(n_bootstraps)):\n",
    "            bootstrap_df = df.sample(frac=1, replace=True)\n",
    "            bootstrap_statistics.append(fxn_to_apply(bootstrap_df, check_consistent_vals_by_group=False, **fxn_kwargs))\n",
    "    if filename is not None:\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump({'point_estimate':point_estimate, 'bootstrap_statistics':bootstrap_statistics}, f)\n",
    "    return point_estimate, bootstrap_statistics\n",
    "\n",
    "# print out a table. Emma reviewed. \n",
    "def create_table_from_bootstrap_results(bootstrap_point_estimate, bootstrap_statistics):\n",
    "    bootstrap_statistics = pd.DataFrame(bootstrap_statistics)\n",
    "    bootstrap_results_table = []\n",
    "    for k in bootstrap_point_estimate.keys():\n",
    "        lower_CI = np.percentile(bootstrap_statistics[k], 2.5)\n",
    "        upper_CI = np.percentile(bootstrap_statistics[k], 97.5)\n",
    "        bootstrap_results_table.append({'quantity':k, \n",
    "                                        'estimate':bootstrap_point_estimate[k], \n",
    "                                        #'bootstrap std':bootstrap_statistics[k].std(), \n",
    "                                        #'2.5% percentile':lower_CI, \n",
    "                                        #'97.5% percentile':upper_CI, \n",
    "                                        'percentile CI':'%2.3f (%2.3f, %2.3f)' % (bootstrap_point_estimate[k], \n",
    "                                                                                         lower_CI, \n",
    "                                                                                         upper_CI), \n",
    "                                       '1.96 sd CI':'%2.3f +/- %2.3f' % (bootstrap_point_estimate[k], 1.96 * bootstrap_statistics[k].std())})\n",
    "    bootstrap_results_table = pd.DataFrame(bootstrap_results_table)\n",
    "    return (bootstrap_results_table.loc[bootstrap_results_table['quantity'].map(lambda x:'relative_to_average' in x), \n",
    "                                       ['quantity', 'estimate', 'percentile CI', '1.96 sd CI']].sort_values(by='estimate')[::-1])\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e4dc419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [44:48<00:00, 134.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>estimate</th>\n",
       "      <th>percentile CI</th>\n",
       "      <th>1.96 sd CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Estimate_Total_Hispanic_or_Latino_relative_to_...</td>\n",
       "      <td>1.049640</td>\n",
       "      <td>1.050 (1.045, 1.055)</td>\n",
       "      <td>1.050 +/- 0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Estimate_Total_Not_Hispanic_or_Latino_Black_or...</td>\n",
       "      <td>1.040333</td>\n",
       "      <td>1.040 (1.035, 1.045)</td>\n",
       "      <td>1.040 +/- 0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estimate_Total_Not_Hispanic_or_Latino_White_al...</td>\n",
       "      <td>0.989513</td>\n",
       "      <td>0.990 (0.985, 0.993)</td>\n",
       "      <td>0.990 +/- 0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Estimate_Total_Not_Hispanic_or_Latino_Asian_al...</td>\n",
       "      <td>0.878738</td>\n",
       "      <td>0.879 (0.872, 0.883)</td>\n",
       "      <td>0.879 +/- 0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            quantity  estimate  \\\n",
       "7  Estimate_Total_Hispanic_or_Latino_relative_to_...  1.049640   \n",
       "6  Estimate_Total_Not_Hispanic_or_Latino_Black_or...  1.040333   \n",
       "5  Estimate_Total_Not_Hispanic_or_Latino_White_al...  0.989513   \n",
       "8  Estimate_Total_Not_Hispanic_or_Latino_Asian_al...  0.878738   \n",
       "\n",
       "          percentile CI       1.96 sd CI  \n",
       "7  1.050 (1.045, 1.055)  1.050 +/- 0.005  \n",
       "6  1.040 (1.035, 1.045)  1.040 +/- 0.006  \n",
       "5  0.990 (0.985, 0.993)  0.990 +/- 0.005  \n",
       "8  0.879 (0.872, 0.883)  0.879 +/- 0.006  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# race table (not conditioning on Zone). Emma reviewed. \n",
    "# compute point estimate and errorbars\n",
    "bootstrap_point_estimate, bootstrap_statistics = bootstrap_function_errorbars(df=d_for_demo_analysis, \n",
    "                             fxn_to_apply=weighted_disparities_estimator, \n",
    "                             fxn_kwargs={'census_area_col':LOCATION_COL_TO_GROUP_ON, \n",
    "                                         'weighting_cols':[WHITE_POPULATION_COL, BLACK_POPULATION_COL, HISPANIC_POPULATION_COL, ASIAN_POPULATION_COL], \n",
    "                                         'total_population_col':TOTAL_POPULATION_COL, \n",
    "                                         'estimate_col':'calibrated_prediction'}, \n",
    "                             n_bootstraps=N_BOOTSTRAPS, \n",
    "                             filename='race_bootstraps.json')\n",
    "create_table_from_bootstrap_results(bootstrap_point_estimate, bootstrap_statistics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c720773f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [32:06<00:00, 96.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>estimate</th>\n",
       "      <th>percentile CI</th>\n",
       "      <th>1.96 sd CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Estimate_Total_Not_Hispanic_or_Latino_Black_or...</td>\n",
       "      <td>1.083814</td>\n",
       "      <td>1.084 (1.080, 1.090)</td>\n",
       "      <td>1.084 +/- 0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Estimate_Total_Hispanic_or_Latino_relative_to_...</td>\n",
       "      <td>1.082602</td>\n",
       "      <td>1.083 (1.077, 1.086)</td>\n",
       "      <td>1.083 +/- 0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estimate_Total_Not_Hispanic_or_Latino_White_al...</td>\n",
       "      <td>0.953256</td>\n",
       "      <td>0.953 (0.948, 0.957)</td>\n",
       "      <td>0.953 +/- 0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Estimate_Total_Not_Hispanic_or_Latino_Asian_al...</td>\n",
       "      <td>0.811473</td>\n",
       "      <td>0.811 (0.806, 0.817)</td>\n",
       "      <td>0.811 +/- 0.007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            quantity  estimate  \\\n",
       "6  Estimate_Total_Not_Hispanic_or_Latino_Black_or...  1.083814   \n",
       "7  Estimate_Total_Hispanic_or_Latino_relative_to_...  1.082602   \n",
       "5  Estimate_Total_Not_Hispanic_or_Latino_White_al...  0.953256   \n",
       "8  Estimate_Total_Not_Hispanic_or_Latino_Asian_al...  0.811473   \n",
       "\n",
       "          percentile CI       1.96 sd CI  \n",
       "6  1.084 (1.080, 1.090)  1.084 +/- 0.006  \n",
       "7  1.083 (1.077, 1.086)  1.083 +/- 0.005  \n",
       "5  0.953 (0.948, 0.957)  0.953 +/- 0.004  \n",
       "8  0.811 (0.806, 0.817)  0.811 +/- 0.007  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# race table CONDITIONING ON ZONE. Emma reviewed. \n",
    "# compute point estimate and errorbars\n",
    "bootstrap_point_estimate, bootstrap_statistics = bootstrap_function_errorbars(\n",
    "    df=d_for_demo_analysis.loc[d_for_demo_analysis['zone'] == 'R'], \n",
    "                             fxn_to_apply=weighted_disparities_estimator, \n",
    "                             fxn_kwargs={'census_area_col':LOCATION_COL_TO_GROUP_ON, \n",
    "                                         'weighting_cols':[WHITE_POPULATION_COL, BLACK_POPULATION_COL, HISPANIC_POPULATION_COL, ASIAN_POPULATION_COL], \n",
    "                                         'total_population_col':TOTAL_POPULATION_COL, \n",
    "                                         'estimate_col':'calibrated_prediction'}, \n",
    "                             n_bootstraps=N_BOOTSTRAPS, \n",
    "                             filename='race_residential_zones_only_bootstraps.json')\n",
    "create_table_from_bootstrap_results(bootstrap_point_estimate, bootstrap_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4941483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of missing values for median_household_income: 0.174104\n",
      "cutoffs for median_household_income [-inf, 51875.0, 77000.0, 115385.0, inf]\n",
      "number of rows in median_household_income_quartile: 3864047\n",
      "number of rows in median_household_income_quartile: 3864508\n",
      "number of rows in median_household_income_quartile: 3865710\n",
      "number of rows in median_household_income_quartile: 3866355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [54:43<00:00, 164.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            quantity  estimate  \\\n",
      "8  median_household_income_quartile_4_relative_to...  1.156263   \n",
      "5  median_household_income_quartile_1_relative_to...  1.122250   \n",
      "7  median_household_income_quartile_3_relative_to...  0.905192   \n",
      "6  median_household_income_quartile_2_relative_to...  0.881983   \n",
      "\n",
      "          percentile CI       1.96 sd CI  \n",
      "8  1.156 (1.145, 1.165)  1.156 +/- 0.011  \n",
      "5  1.122 (1.110, 1.132)  1.122 +/- 0.012  \n",
      "7  0.905 (0.899, 0.915)  0.905 +/- 0.010  \n",
      "6  0.882 (0.875, 0.888)  0.882 +/- 0.009  \n"
     ]
    }
   ],
   "source": [
    "# continuous variables: divide into quartile. Emma reviewed. \n",
    "\n",
    "# density_cbg, median_household_income\n",
    "\n",
    "for col in ['median_household_income']: # ['density_cbg']\n",
    "    percentile_cutoffs = [25, 50, 75]\n",
    "    print(\"Fraction of missing values for %s: %2.6f\" % (col, d_for_demo_analysis[col].isnull().mean()))\n",
    "    d_for_col = d_for_demo_analysis.dropna(subset=[col]).copy()\n",
    "    cutoff_vals = np.percentile(d_for_col[col], percentile_cutoffs)\n",
    "    cutoff_vals = [-np.inf] + list(cutoff_vals) + [np.inf]\n",
    "    print('cutoffs for %s' % col, cutoff_vals)\n",
    "    d_for_col['%s_quartile' % col] = None\n",
    "\n",
    "    for i in range(len(cutoff_vals) - 1):\n",
    "        quartile_idxs = d_for_col[col].map(lambda x:(x >= cutoff_vals[i]) & (x < cutoff_vals[i + 1]))\n",
    "        d_for_col.loc[quartile_idxs, '%s_quartile' % col] = '%s_quartile_%i' % (col, i + 1)\n",
    "        print('number of rows in %s: %i' % ('%s_quartile' % col, quartile_idxs.sum()))\n",
    "    bootstrap_point_estimate, bootstrap_statistics = bootstrap_function_errorbars(df=d_for_col, \n",
    "                             fxn_to_apply=weighted_disparities_estimator_two_level_grouping, \n",
    "                             fxn_kwargs={'census_area_col':LOCATION_COL_TO_GROUP_ON, \n",
    "                                         'high_level_group_col':'%s_quartile' % col,\n",
    "                                         'total_population_col':TOTAL_POPULATION_COL, \n",
    "                                         'estimate_col':'calibrated_prediction'}, \n",
    "                             n_bootstraps=N_BOOTSTRAPS, \n",
    "                             filename='%s_bootstraps.json' % col)\n",
    "\n",
    "    print(create_table_from_bootstrap_results(bootstrap_point_estimate, bootstrap_statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa2add0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [1:02:14<00:00, 186.75s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>estimate</th>\n",
       "      <th>percentile CI</th>\n",
       "      <th>1.96 sd CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_relative_to_average</td>\n",
       "      <td>1.985214</td>\n",
       "      <td>1.985 (1.949, 2.005)</td>\n",
       "      <td>1.985 +/- 0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M_relative_to_average</td>\n",
       "      <td>0.948387</td>\n",
       "      <td>0.948 (0.930, 0.968)</td>\n",
       "      <td>0.948 +/- 0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R_relative_to_average</td>\n",
       "      <td>0.937805</td>\n",
       "      <td>0.938 (0.937, 0.940)</td>\n",
       "      <td>0.938 +/- 0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                quantity  estimate         percentile CI       1.96 sd CI\n",
       "4  C_relative_to_average  1.985214  1.985 (1.949, 2.005)  1.985 +/- 0.033\n",
       "5  M_relative_to_average  0.948387  0.948 (0.930, 0.968)  0.948 +/- 0.019\n",
       "6  R_relative_to_average  0.937805  0.938 (0.937, 0.940)  0.938 +/- 0.002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zone table. Emma reviewed. \n",
    "bootstrap_point_estimate, bootstrap_statistics = bootstrap_function_errorbars(df=d_for_demo_analysis, \n",
    "                             fxn_to_apply=weighted_disparities_estimator_two_level_grouping, \n",
    "                             fxn_kwargs={'census_area_col':LOCATION_COL_TO_GROUP_ON, \n",
    "                                         'high_level_group_col':'zone',\n",
    "                                         'total_population_col':TOTAL_POPULATION_COL, \n",
    "                                         'estimate_col':'calibrated_prediction'}, \n",
    "                             n_bootstraps=N_BOOTSTRAPS, \n",
    "                             filename='zone_bootstraps.json')\n",
    "\n",
    "create_table_from_bootstrap_results(bootstrap_point_estimate, bootstrap_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a7dead1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [1:04:47<00:00, 194.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>estimate</th>\n",
       "      <th>percentile CI</th>\n",
       "      <th>1.96 sd CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Manhattan_relative_to_average</td>\n",
       "      <td>1.619059</td>\n",
       "      <td>1.619 (1.610, 1.625)</td>\n",
       "      <td>1.619 +/- 0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bronx_relative_to_average</td>\n",
       "      <td>0.994406</td>\n",
       "      <td>0.994 (0.983, 1.007)</td>\n",
       "      <td>0.994 +/- 0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brooklyn_relative_to_average</td>\n",
       "      <td>0.932224</td>\n",
       "      <td>0.932 (0.926, 0.937)</td>\n",
       "      <td>0.932 +/- 0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Queens_relative_to_average</td>\n",
       "      <td>0.738071</td>\n",
       "      <td>0.738 (0.732, 0.744)</td>\n",
       "      <td>0.738 +/- 0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Staten Island_relative_to_average</td>\n",
       "      <td>0.507765</td>\n",
       "      <td>0.508 (0.495, 0.521)</td>\n",
       "      <td>0.508 +/- 0.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             quantity  estimate         percentile CI  \\\n",
       "8       Manhattan_relative_to_average  1.619059  1.619 (1.610, 1.625)   \n",
       "6           Bronx_relative_to_average  0.994406  0.994 (0.983, 1.007)   \n",
       "7        Brooklyn_relative_to_average  0.932224  0.932 (0.926, 0.937)   \n",
       "9          Queens_relative_to_average  0.738071  0.738 (0.732, 0.744)   \n",
       "10  Staten Island_relative_to_average  0.507765  0.508 (0.495, 0.521)   \n",
       "\n",
       "         1.96 sd CI  \n",
       "8   1.619 +/- 0.008  \n",
       "6   0.994 +/- 0.014  \n",
       "7   0.932 +/- 0.007  \n",
       "9   0.738 +/- 0.007  \n",
       "10  0.508 +/- 0.016  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boro table. Emma reviewed. \n",
    "bootstrap_point_estimate, bootstrap_statistics = bootstrap_function_errorbars(df=d_for_demo_analysis, \n",
    "                             fxn_to_apply=weighted_disparities_estimator_two_level_grouping, \n",
    "                             fxn_kwargs={'census_area_col':LOCATION_COL_TO_GROUP_ON, \n",
    "                                         'high_level_group_col':'boroname',\n",
    "                                         'total_population_col':TOTAL_POPULATION_COL, \n",
    "                                         'estimate_col':'calibrated_prediction'}, \n",
    "                             n_bootstraps=N_BOOTSTRAPS, \n",
    "                             filename='boro_bootstraps.json')\n",
    "\n",
    "create_table_from_bootstrap_results(bootstrap_point_estimate, bootstrap_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4ee035",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Astoria Park because total population is 0\n",
      "Skipping Brooklyn Navy Yard because total population is 0\n",
      "Skipping Calvary & Mount Zion Cemeteries because total population is 0\n",
      "Skipping Canarsie Park & Pier because total population is 0\n",
      "Skipping Claremont Park because total population is 0\n",
      "Skipping Crotona Park because total population is 0\n",
      "Skipping Cunningham Park because total population is 0\n",
      "Skipping Ferry Point Park-St. Raymond Cemetery because total population is 0\n",
      "Skipping Flushing Meadows-Corona Park because total population is 0\n",
      "Skipping Forest Park because total population is 0\n",
      "Skipping Fort Totten because total population is 0\n",
      "Skipping Freshkills Park (North) because total population is 0\n",
      "Skipping Great Kills Park because total population is 0\n",
      "Skipping Green-Wood Cemetery because total population is 0\n",
      "Skipping Highbridge Park because total population is 0\n",
      "Skipping Highland Park-Cypress Hills Cemeteries (South) because total population is 0\n",
      "Skipping John F. Kennedy International Airport because total population is 0\n",
      "Skipping Kissena Park because total population is 0\n",
      "Skipping LaGuardia Airport because total population is 0\n",
      "Skipping Lincoln Terrace Park because total population is 0\n",
      "Skipping Marine Park-Plumb Island because total population is 0\n",
      "Skipping McGuire Fields because total population is 0\n",
      "Skipping Middle Village Cemetery because total population is 0\n",
      "Skipping Montefiore Cemetery because total population is 0\n",
      "Skipping Mount Hebron & Cedar Grove Cemeteries because total population is 0\n",
      "Skipping Prospect Park because total population is 0\n",
      "Skipping Rockaway Community Park because total population is 0\n",
      "Skipping Shirley Chisholm State Park because total population is 0\n",
      "Skipping Snug Harbor because total population is 0\n",
      "Skipping Soundview Park because total population is 0\n",
      "Skipping Spring Creek Park because total population is 0\n",
      "Skipping St. John Cemetery because total population is 0\n",
      "Skipping St. Michael's Cemetery because total population is 0\n",
      "Skipping Sunnyside Yards (South) because total population is 0\n",
      "Skipping The Battery-Governors Island-Ellis Island-Liberty Island because total population is 0\n",
      "Skipping The Evergreens Cemetery because total population is 0\n",
      "Skipping United Nations because total population is 0\n",
      "Skipping Van Cortlandt Park because total population is 0\n",
      "Skipping Woodlawn Cemetery because total population is 0\n",
      "Skipping Yankee Stadium-Macombs Dam Park because total population is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                                                                                        | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Astoria Park because total population is 0\n",
      "Skipping Brooklyn Navy Yard because total population is 0\n",
      "Skipping Calvary & Mount Zion Cemeteries because total population is 0\n",
      "Skipping Canarsie Park & Pier because total population is 0\n",
      "Skipping Claremont Park because total population is 0\n",
      "Skipping Crotona Park because total population is 0\n",
      "Skipping Cunningham Park because total population is 0\n",
      "Skipping Ferry Point Park-St. Raymond Cemetery because total population is 0\n",
      "Skipping Flushing Meadows-Corona Park because total population is 0\n",
      "Skipping Forest Park because total population is 0\n",
      "Skipping Fort Totten because total population is 0\n",
      "Skipping Freshkills Park (North) because total population is 0\n",
      "Skipping Great Kills Park because total population is 0\n",
      "Skipping Green-Wood Cemetery because total population is 0\n",
      "Skipping Highbridge Park because total population is 0\n",
      "Skipping Highland Park-Cypress Hills Cemeteries (South) because total population is 0\n",
      "Skipping John F. Kennedy International Airport because total population is 0\n",
      "Skipping Kissena Park because total population is 0\n",
      "Skipping LaGuardia Airport because total population is 0\n",
      "Skipping Lincoln Terrace Park because total population is 0\n",
      "Skipping Marine Park-Plumb Island because total population is 0\n",
      "Skipping McGuire Fields because total population is 0\n",
      "Skipping Middle Village Cemetery because total population is 0\n",
      "Skipping Montefiore Cemetery because total population is 0\n",
      "Skipping Mount Hebron & Cedar Grove Cemeteries because total population is 0\n",
      "Skipping Prospect Park because total population is 0\n",
      "Skipping Rockaway Community Park because total population is 0\n",
      "Skipping Shirley Chisholm State Park because total population is 0\n",
      "Skipping Snug Harbor because total population is 0\n",
      "Skipping Soundview Park because total population is 0\n",
      "Skipping Spring Creek Park because total population is 0\n",
      "Skipping St. John Cemetery because total population is 0\n",
      "Skipping St. Michael's Cemetery because total population is 0\n",
      "Skipping Sunnyside Yards (South) because total population is 0\n",
      "Skipping The Battery-Governors Island-Ellis Island-Liberty Island because total population is 0\n",
      "Skipping The Evergreens Cemetery because total population is 0\n",
      "Skipping United Nations because total population is 0\n",
      "Skipping Van Cortlandt Park because total population is 0\n",
      "Skipping Woodlawn Cemetery because total population is 0\n",
      "Skipping Yankee Stadium-Macombs Dam Park because total population is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  5%|██████▎                                                                                                                      | 1/20 [04:15<1:20:50, 255.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Astoria Park because total population is 0\n",
      "Skipping Brooklyn Navy Yard because total population is 0\n",
      "Skipping Calvary & Mount Zion Cemeteries because total population is 0\n",
      "Skipping Canarsie Park & Pier because total population is 0\n",
      "Skipping Claremont Park because total population is 0\n",
      "Skipping Crotona Park because total population is 0\n",
      "Skipping Cunningham Park because total population is 0\n",
      "Skipping Ferry Point Park-St. Raymond Cemetery because total population is 0\n",
      "Skipping Flushing Meadows-Corona Park because total population is 0\n",
      "Skipping Forest Park because total population is 0\n",
      "Skipping Fort Totten because total population is 0\n",
      "Skipping Freshkills Park (North) because total population is 0\n",
      "Skipping Great Kills Park because total population is 0\n",
      "Skipping Green-Wood Cemetery because total population is 0\n",
      "Skipping Highbridge Park because total population is 0\n",
      "Skipping Highland Park-Cypress Hills Cemeteries (South) because total population is 0\n",
      "Skipping John F. Kennedy International Airport because total population is 0\n",
      "Skipping Kissena Park because total population is 0\n",
      "Skipping LaGuardia Airport because total population is 0\n",
      "Skipping Lincoln Terrace Park because total population is 0\n",
      "Skipping Marine Park-Plumb Island because total population is 0\n",
      "Skipping McGuire Fields because total population is 0\n",
      "Skipping Middle Village Cemetery because total population is 0\n",
      "Skipping Montefiore Cemetery because total population is 0\n",
      "Skipping Mount Hebron & Cedar Grove Cemeteries because total population is 0\n",
      "Skipping Prospect Park because total population is 0\n",
      "Skipping Rockaway Community Park because total population is 0\n",
      "Skipping Shirley Chisholm State Park because total population is 0\n",
      "Skipping Snug Harbor because total population is 0\n",
      "Skipping Soundview Park because total population is 0\n",
      "Skipping Spring Creek Park because total population is 0\n",
      "Skipping St. John Cemetery because total population is 0\n",
      "Skipping St. Michael's Cemetery because total population is 0\n",
      "Skipping Sunnyside Yards (South) because total population is 0\n",
      "Skipping The Battery-Governors Island-Ellis Island-Liberty Island because total population is 0\n",
      "Skipping The Evergreens Cemetery because total population is 0\n",
      "Skipping United Nations because total population is 0\n",
      "Skipping Van Cortlandt Park because total population is 0\n",
      "Skipping Woodlawn Cemetery because total population is 0\n",
      "Skipping Yankee Stadium-Macombs Dam Park because total population is 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████████▌                                                                                                                | 2/20 [10:52<1:37:50, 326.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/share/pierson/tmp_directory_location_please_read_readme/ep432_tmp/ipykernel_81572/3381072744.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# neighborhood table. Emma reviewed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m bootstrap_point_estimate, bootstrap_statistics = bootstrap_function_errorbars(df=d_for_demo_analysis, \n\u001b[0m\u001b[1;32m      3\u001b[0m                              \u001b[0mfxn_to_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweighted_disparities_estimator_two_level_grouping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                              fxn_kwargs={'census_area_col':LOCATION_COL_TO_GROUP_ON, \n\u001b[1;32m      5\u001b[0m                                          \u001b[0;34m'high_level_group_col'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mNEIGHBORHOOD_COL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pierson/tmp_directory_location_please_read_readme/ep432_tmp/ipykernel_81572/2107303646.py\u001b[0m in \u001b[0;36mbootstrap_function_errorbars\u001b[0;34m(df, fxn_to_apply, fxn_kwargs, n_bootstraps, filename)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mpoint_estimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfxn_to_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_consistent_vals_by_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfxn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbootstrap\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bootstraps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mbootstrap_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mbootstrap_statistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfxn_to_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrap_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_consistent_vals_by_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfxn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pierson/conda_virtualenvs/nytimes_conda_env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   5349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5350\u001b[0m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5353\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pierson/conda_virtualenvs/nytimes_conda_env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3586\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   3587\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3588\u001b[0m         )\n",
      "\u001b[0;32m/share/pierson/conda_virtualenvs/nytimes_conda_env/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m   1475\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m         )\n",
      "\u001b[0;32m/share/pierson/conda_virtualenvs/nytimes_conda_env/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   1309\u001b[0m             )\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m             new_blocks = [\n\u001b[0m\u001b[1;32m   1312\u001b[0m                 blk.take_nd(\n\u001b[1;32m   1313\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pierson/conda_virtualenvs/nytimes_conda_env/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m             new_blocks = [\n\u001b[0;32m-> 1312\u001b[0;31m                 blk.take_nd(\n\u001b[0m\u001b[1;32m   1313\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pierson/conda_virtualenvs/nytimes_conda_env/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1393\u001b[0m             \u001b[0mallow_fill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m         new_values = algos.take_nd(\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         )\n",
      "\u001b[0;32m/share/pierson/conda_virtualenvs/nytimes_conda_env/lib/python3.8/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1757\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m     )\n\u001b[0;32m-> 1759\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# neighborhood table. Emma reviewed. \n",
    "bootstrap_point_estimate, bootstrap_statistics = bootstrap_function_errorbars(df=d_for_demo_analysis, \n",
    "                             fxn_to_apply=weighted_disparities_estimator_two_level_grouping, \n",
    "                             fxn_kwargs={'census_area_col':LOCATION_COL_TO_GROUP_ON, \n",
    "                                         'high_level_group_col':NEIGHBORHOOD_COL,\n",
    "                                         'total_population_col':TOTAL_POPULATION_COL, \n",
    "                                         'estimate_col':'calibrated_prediction'}, \n",
    "                             n_bootstraps=N_BOOTSTRAPS, \n",
    "                             filename='neighborhood_bootstraps.json')\n",
    "\n",
    "create_table_from_bootstrap_results(bootstrap_point_estimate, bootstrap_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc0de1",
   "metadata": {},
   "source": [
    "# Estimator described in Google doc applied to estimate disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction_col in PREDICTION_COLS:\n",
    "    print(\"Using prediction col\", prediction_col)\n",
    "    estimates = {}\n",
    "    for demo_col in POPULATION_COUNT_COLS:\n",
    "        if demo_col == TOTAL_POPULATION_COL:\n",
    "            continue\n",
    "        # compute weighted mean as described in Census tract. \n",
    "        grouped_mean = (grouped_d[prediction_col] * grouped_d[demo_col]).sum()/grouped_d[demo_col].sum()\n",
    "        print(demo_col, grouped_mean)\n",
    "        estimates[demo_col] = grouped_mean\n",
    "    print(\"Ratio of Black estimate to white estimate: %2.3f\" % (estimates[BLACK_POPULATION_COL]/estimates[WHITE_POPULATION_COL]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345b3e3",
   "metadata": {},
   "source": [
    "# scatterplots\n",
    "\n",
    "Basically these plot Pr(police car) by Census area against various demographic variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061c66f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for var in DEMOGRAPHIC_COLS:\n",
    "    for prediction_col in PREDICTION_COLS:\n",
    "        plt.figure(figsize=[4, 4])\n",
    "        # in making these scatterplots, we drop very small Census areas (smaller than MIN_POPULATION_IN_AREA)\n",
    "        # just to avoid very noisy estimates. \n",
    "        # The weighted correlation measure verifies that this doesn't change results a lot. \n",
    "        d_to_plot = grouped_d.loc[grouped_d[TOTAL_POPULATION_COL] > MIN_POPULATION_IN_AREA].dropna(subset=[var, prediction_col])\n",
    "        plt.scatter(d_to_plot[var], \n",
    "                    d_to_plot[prediction_col], \n",
    "                    s=d_to_plot[TOTAL_POPULATION_COL] * 1e-3)\n",
    "        plt.xlabel(var, fontsize=16)\n",
    "        plt.ylabel(prediction_col, fontsize=16)\n",
    "        pearson_r, pearson_p = pearsonr(d_to_plot[var], d_to_plot[prediction_col])\n",
    "        spearman_r, spearman_p = spearmanr(d_to_plot[var], d_to_plot[prediction_col])\n",
    "        d_for_weighted_correlation = grouped_d.dropna(subset=[var, prediction_col])\n",
    "        weighted_correlation = DescrStatsW(d_for_weighted_correlation[[var, prediction_col]], \n",
    "                                           weights=d_for_weighted_correlation[TOTAL_POPULATION_COL]).corrcoef[0][1]\n",
    "        plt.title('pearsonr %2.3f, p=%2.3e (population-weighted %2.3f)\\nspearmanr %2.3f, p=%2.3e' % (pearson_r, pearson_p, weighted_correlation, spearman_r, spearman_p))\n",
    "        plt.ylim([0, 0.05])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc52e086",
   "metadata": {},
   "source": [
    "# correlations between all measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_d.loc[grouped_d[TOTAL_POPULATION_COL] > MIN_POPULATION_IN_AREA, DEMOGRAPHIC_COLS].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec64b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_d.loc[grouped_d[TOTAL_POPULATION_COL] > MIN_POPULATION_IN_AREA, DEMOGRAPHIC_COLS].corr(method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfc1c3",
   "metadata": {},
   "source": [
    "# broken down by neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c36f0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in PREDICTION_COLS:\n",
    "    print(\"\\n\\nneighborhoods with highest mean values of %s\" % col)\n",
    "    print(d_for_demo_analysis\n",
    "          .groupby(NEIGHBORHOOD_COL)[col]\n",
    "          .agg(['mean', 'size'])\n",
    "          .reset_index()\n",
    "          .sort_values(by='mean')[::-1]\n",
    "          .head(n=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in PREDICTION_COLS:\n",
    "    print(\"\\n\\nneighborhoods with highest mean values of %s\" % col)\n",
    "    print(d_for_demo_analysis\n",
    "          .groupby(BOROUGH_COL)[col]\n",
    "          .agg(['mean', 'size'])\n",
    "          .reset_index().sort_values(by='mean')[::-1]\n",
    "          .head(n=50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nytimes_conda_env] *",
   "language": "python",
   "name": "conda-env-nytimes_conda_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
